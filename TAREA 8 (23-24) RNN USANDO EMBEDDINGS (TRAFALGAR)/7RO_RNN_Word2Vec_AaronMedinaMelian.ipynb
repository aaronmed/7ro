{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# Laboratorio: Modelos del lenguaje con RNNs (Word2Vec)\n",
        "\n",
        "[Enlace al cuaderno en el Github](https://github.com/aaronmed/7ro/blob/master/TAREA%208%20(23-24)%20RNN%20USANDO%20EMBEDDINGS%20(TRAFALGAR)/7RO_RNN_Word2Vec_AaronMedinaMelian.ipynb)\n",
        "\n",
        "Aarón Medina Melián\n",
        "\n",
        "En este laboratorio, vamos a entrenar un modelo del lenguaje basado en caracteres con Recurrent Neural Networks. Asimismo, utilizaremos el modelo para generar texto. En particular, alimentaremos nuestro modelo con obras de la literatura clásica en castellano para obtener una red neuronal que sea capaz de \"escribir\" fragmentos literarios.\n",
        "\n",
        "Los entrenamientos en esta laboratorio para obtener un modelo de calidad podrían tomar cierto tiempo (5-10 minutos por epoch), por lo que se aconseja empezar a trabajar pronto. El uso de GPUs no ayuda tanto con LSTMs como con CNNs, por lo que si tenéis máquinas potentes en casa es posible que podáis entrenar más rápido o a la misma velocidad que en Colab. En todo caso, la potencia de Colab es más que suficiente para completar este laboratorio con éxito.\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Portada_Trafalgar_%281873%29.jpg/800px-Portada_Trafalgar_%281873%29.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consistirá en un archivo de texto con el contenido íntegro en castellano de Trafalgar, disponible de manera libre en la página de [Project Gutenberg](https://www.gutenberg.org). Asimismo, como apartado optativo en este laboratorio se pueden utilizar otras fuentes de texto. Aquí podéis descargar los datos a utilizar de El Quijote y un par de obras adicionales:\n",
        "\n",
        "[El ingenioso hidalgo Don Quijote de la Mancha (Miguel de Cervantes)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io)\n",
        "\n",
        "[Compilación de obras teatrales (Calderón de la Barca)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219433&authkey=AKvGD6DC3IRBqmc)\n",
        "\n",
        "[Trafalgar (Benito Pérez Galdós)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ)\n",
        "\n",
        "Como ya deberíamos de estar acostumbrados en problemas de Machine Learning, es importante echar un vistazo a los datos antes de empezar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## 1. Carga y procesado del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZNnzvXuqVVm"
      },
      "source": [
        "Primero, vamos a descargar el libro e inspeccionar los datos. El fichero a descargar es una versión en .txt del libro Trafalgar, a la cual se le han borrado introducciones, licencias y otras secciones para dejarlo con el contenido real de la novela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import random\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D7tKOZ9BFfki",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "path = keras.utils.get_file(fname=\"trafalgar.txt\", origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGLvjLXrUUd"
      },
      "source": [
        "Una vez descargado, vamos a leer el contenido del fichero en una variable. Adicionalmente, convertiremos el contenido del texto a minúsculas para ponérselo un poco más fácil a nuestro modelo (de modo que todas las letras sean minúsculas y el modelo no necesite diferenciar entre minúsculas y mayúsculas).\n",
        "\n",
        "**1.1.** Leer todo el contenido del fichero en una única variable ***text*** y convertir el string a minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8WB6FejrrTu9",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "text = open(path, encoding=\"utf8\").read().lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkgGl8GWtUk8"
      },
      "source": [
        "Podemos comprobar ahora que efectivamente nuestra variable contiene el resultado deseado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hMFhe3COFwSD",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del texto: 300039\n",
            "-i-\n",
            "\n",
            "se me permitirá que antes de referir el gran suceso de que fui testigo,\n",
            "diga algunas palabras sobre mi infancia, explicando por qué extraña\n",
            "manera me llevaron los azares de la vida a presenciar la terrible\n",
            "catástrofe de nuestra marina.\n",
            "\n",
            "al hablar de mi nacimiento, no imitaré a la mayor parte de\n"
          ]
        }
      ],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "print(\"Longitud del texto: {}\".format(len(text)))\n",
        "print(text[0:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "text = text[5:len(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n",
            "299920\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'fin de trafalgar\\n\\nmadrid, enero-febrero 1873.\\n\\nbenito pérez galdós; edición ilustrada por enrique y arturo mélida\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fin = \"fin de trafalgar\"\n",
        "posicion = text.find(fin)\n",
        "\n",
        "\n",
        "print(len(fin))\n",
        "print(posicion)\n",
        "\n",
        "text[posicion:len(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "text = text[0:posicion+len(fin)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7TUXWiyvOj"
      },
      "source": [
        "## 2. Procesado de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x66_Vi_Gyxns"
      },
      "source": [
        "Una de las grandes ventajas de trabajar con modelos que utilizan caracteres en vez de palabras es que no necesitamos tokenizar el texto (partirlo palabra a palabra). Nuestro modelo funcionará directamente con los caracteres en el texto, incluyendo espacios, saltos de línea, etc.\n",
        "\n",
        "Antes de hacer nada, necesitamos procesar el texto en entradas y salidas compatibles con nuestro modelo. Como sabemos, un modelo del lenguaje con RNNs acepta una serie de caracteres y predice el siguiente carácter en la secuencia.\n",
        "\n",
        "* \"*Se me permitirá que antes de referir el gr*\" -> predicción: **a**\n",
        "* \"*e me permitirá que antes de referir el gra*\" -> predicción: **n**\n",
        "\n",
        "De modo que la entrada y la salida de nuestro modelo necesita ser algo parecido a este esquema. En este punto, podríamos usar dos formas de preparar los datos para nuestro modelo.\n",
        "\n",
        "1. **Secuencia a secuencia**. La entrada de nuestro modelo sería una secuencia y la salida sería esa secuencia trasladada un caracter a la derecha, de modo que en cada instante de tiempo la RNN tiene que predecir el carácter siguiente. Por ejemplo:\n",
        "\n",
        ">* *Input*:  Se me permitirá que antes de referir el gr\n",
        ">* *Output*: e me permitirá que antes de referir el gra\n",
        "\n",
        "2. **Secuencia a carácter**. En este variante, pasaríamos una secuencia de caracteres por nuestra RNN y, al llegar al final de la secuencia, predeciríamos el siguiente carácter.\n",
        "\n",
        ">* *Input*:  Se me permitirá que antes de referir el gr\n",
        ">* *Output*: a\n",
        "\n",
        "En este laboratorio, por simplicidad, vamos a utilizar la segunda variante (PERO USANDO PALABRAS NO CARACTERES).\n",
        "\n",
        "De este modo, a partir del texto, hemos de generar nuestro propio training data que consista en secuencias de PALABRAS con la siguiente PALABRA a predecir. Para estandarizar las cosas, utilizaremos secuencias de tamaño *SEQ_LENGTH* PALABRAS (un hiperparámetro que podemos elegir nosotros).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfJUIxW5m5C"
      },
      "source": [
        "#### 2.1. Obtención de las palabras y mapas de palabras\n",
        "\n",
        "Antes que nada, necesitamos saber qué palabras aparecen en el texto, ya que tendremos que diferenciarlos mediante un índice de 0 a *num_words* - 1 en el modelo. Obtener:\n",
        "\n",
        "\n",
        "1.   Número de caracteres únicos que aparecen en el texto.\n",
        "2.   Diccionario que asocia char a índice único entre 0 y *num_words* - 1. Por ejemplo, {'se': 0, 'me': 1, ...}\n",
        "3.   Diccionario reverso de índices a palabras: {0: 'se', 1: 'me', ...}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def quitar_tildes_y_mantener_n(texto):\n",
        "    # Reemplazar letras acentuadas con sus equivalentes sin acento\n",
        "    texto_sin_tildes = ''\n",
        "    for caracter in texto:\n",
        "        if caracter == 'ñ' or caracter == 'Ñ':\n",
        "            texto_sin_tildes += caracter\n",
        "        else:\n",
        "            texto_sin_tildes += unidecode(caracter)\n",
        "    return texto_sin_tildes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = re.sub(r'[^A-Za-záéíóúüÁÉÍÓÚÜñÑ\\s]', '', text)\n",
        "# Sustituir letras con tilde por su versión sin tilde\n",
        "text = quitar_tildes_y_mantener_n(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5bJ0NsbCbupF"
      },
      "outputs": [],
      "source": [
        "palabras = text.split()\n",
        "palabras_unicas = set(palabras)\n",
        "word_index = {}\n",
        "k = 0\n",
        "for palabra in palabras_unicas:\n",
        "    word_index[palabra] = k\n",
        "    k+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "reversed_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_B4AWo0ElwA"
      },
      "source": [
        "#### 2.2. Obtención de secuencias de entrada y palabra a predecir\n",
        "\n",
        "Ahora, vamos a obtener las secuencias de entrada en formato texto y las correspondientes palabras a predecir. Para ello, recorrer el texto completo leído anteriormente, obteniendo una secuencia de SEQ_LENGTH palabras y la siguiente palabra a predecir. Una vez hecho, desplazarse una palabra a la derecha y hacer lo mismo para obtener una nueva secuencia y predicción. Guardar las secuencias en una variable ***sequences*** y las palabras a predecir en una variable ***next_words***.\n",
        "\n",
        "Por ejemplo, si el texto fuera \"Don Quijote\" y SEQ_LENGTH fuese 2, tendríamos\n",
        "\n",
        "* *sequences* = [\"Don Quijote\", \"Quijote de\"]\n",
        "* *next_chars* = ['de', 'La']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NslxhnnDK6uA"
      },
      "outputs": [],
      "source": [
        "# Definimos el tamaño de las secuencias. Puedes dejar este valor por defecto.\n",
        "SEQ_LENGTH = 5\n",
        "step = 1\n",
        "sequences = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(0,len(palabras)-SEQ_LENGTH, step):\n",
        "  sequences.append(palabras[i:i+SEQ_LENGTH])\n",
        "  next_words.append(palabras[i+SEQ_LENGTH])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplo: secuencia número 5:\n",
            "['antes', 'de', 'referir', 'el', 'gran']\n",
            "Siguiente caracter:\n",
            "suceso\n"
          ]
        }
      ],
      "source": [
        "print('Ejemplo: secuencia número 5:')\n",
        "print(sequences[4])\n",
        "print('Siguiente caracter:')\n",
        "print(next_words[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y3AmjYtHdLJ"
      },
      "source": [
        "Indicar el tamaño del training set que acabamos de generar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WVWqKxFcbwTu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "número de datos de training: 51039\n"
          ]
        }
      ],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "print('número de datos de training: {}'.format(len(sequences)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGQkKcwpLRJ"
      },
      "source": [
        "Como el libro es muy largo y tenemos muchas secuencias, podríamos encontrar problemas de memoria. Por ello, vamos a elegir un número máximo de ellas. Si estás corriendo esto localmente y tienes problemas de memoria, puedes reducir el tamaño aún más, pero ten cuidado porque, a menos datos, peor calidad del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2pm1Q19ppw8F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n"
          ]
        }
      ],
      "source": [
        "MAX_SEQUENCES = 50000\n",
        "\n",
        "perm = np.random.permutation(len(sequences))\n",
        "sequences, next_words = np.array(sequences), np.array(next_words)\n",
        "sequences, next_words = sequences[perm], next_words[perm]\n",
        "sequences, next_words = list(sequences[:MAX_SEQUENCES]), list(next_words[:MAX_SEQUENCES])\n",
        "\n",
        "print(len(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['entre', 'estos', 'el', 'que', 'mas'], dtype='<U21')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences[len(sequences)-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'come'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next_words[len(sequences)-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Aaron\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "frases = []\n",
        "\n",
        "for i in range(len(sequences)):\n",
        "    frase = ' '.join(sequences[i])\n",
        "    frases.append(frase)\n",
        "newsVec = [nltk.word_tokenize(frase) for frase in frases]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "modelWord2Vec = Word2Vec(newsVec, min_count = 1, vector_size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.00536505, -0.18918887,  0.16775326,  0.10664421, -0.04213215,\n",
              "       -0.16734448,  0.09119376,  0.07776405, -0.02377144, -0.04970309,\n",
              "        0.12521765, -0.21872485, -0.01353685, -0.02692703, -0.04997571,\n",
              "       -0.04542817, -0.02418974,  0.07634109,  0.06548865,  0.1887923 ,\n",
              "        0.23657173,  0.2844065 ,  0.35218358, -0.09725776,  0.12133976,\n",
              "        0.01982906, -0.19485554,  0.09789208, -0.000523  , -0.15865903,\n",
              "       -0.06777231,  0.045249  ], dtype=float32)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelWord2Vec.wv[\"orgullosa\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('imagenes', 0.9922328591346741),\n",
              " ('epoca', 0.9918032884597778),\n",
              " ('hacerme', 0.9911080598831177),\n",
              " ('dibujos', 0.9910831451416016),\n",
              " ('alto', 0.990997314453125),\n",
              " ('embarcacion', 0.9909549355506897),\n",
              " ('avanzada', 0.9909378290176392),\n",
              " ('derecho', 0.9908908605575562),\n",
              " ('union', 0.9907886981964111),\n",
              " ('furioso', 0.9907875061035156)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelWord2Vec.wv.most_similar(\"orgullosa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FzgtAbPIs6f"
      },
      "source": [
        "#### 2.3. Obtención de input X y output y para el modelo\n",
        "\n",
        "Finalmente, a partir de los datos de entrenamiento que hemos generado vamos a crear los arrays de datos X e y que pasaremos a nuestro modelo.\n",
        "\n",
        "Para ello, vamos a utilizar *one-hot encoding* para nuestras palabras. Por ejemplo, si sólo tuviéramos 4 palabras (a, b, c, d), las representaciones serían: (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0) y (0, 0, 0, 1).\n",
        "\n",
        "De este modo, **X** tendrá shape *(num_sequences, seq_length, num_words)* e **y** tendrá shape *(num_sequences, num_words)*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_SEQUENCES = len(sequences)\n",
        "X = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, 32))\n",
        "y = np.zeros((NUM_SEQUENCES, len(palabras_unicas)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in range(NUM_SEQUENCES):\n",
        "    palabra = next_words[k]\n",
        "    y[k, word_index[palabra]] = 1 #codificación ONE-HOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k in range(NUM_SEQUENCES):\n",
        "    for w in range(SEQ_LENGTH):\n",
        "        word = sequences[k][w]\n",
        "        X[k, w] = modelWord2Vec.wv[word]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeUxz3HPm3l"
      },
      "source": [
        "## 3. Definición del modelo y entrenamiento\n",
        "\n",
        "Una vez tenemos ya todo preparado, es hora de definir el modelo. Define un modelo que utilice una **LSTM** con **128 unidades internas**. Si bien el modelo puede definirse de una manera más compleja, para empezar debería bastar con una LSTM más una capa Dense con el *softmax* que predice el siguiente caracter a producir. Adam puede ser una buena elección de optimizador.\n",
        "\n",
        "Una vez el modelo esté definido, entrénalo un poco para asegurarte de que la loss es decreciente. No es necesario guardar la salida de este entrenamiento en el entregable final, ya que vamos a hacer el entrenamiento más informativo en el siguiente punto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "vocabulary_size = len(palabras_unicas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MSw2j0btYWZs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 128)               82432     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8487)              1094823   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1177255 (4.49 MB)\n",
            "Trainable params: 1177255 (4.49 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQ_LENGTH, 32)))\n",
        "model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "196/196 [==============================] - 11s 50ms/step - loss: 7.1861 - accuracy: 0.0449 - val_loss: 6.9072 - val_accuracy: 0.0506\n",
            "Epoch 2/40\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 6.5920 - accuracy: 0.0535 - val_loss: 6.9836 - val_accuracy: 0.0672\n",
            "Epoch 3/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 6.5069 - accuracy: 0.0679 - val_loss: 7.0009 - val_accuracy: 0.0687\n",
            "Epoch 4/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 6.4081 - accuracy: 0.0708 - val_loss: 6.9460 - val_accuracy: 0.0729\n",
            "Epoch 5/40\n",
            "196/196 [==============================] - 8s 39ms/step - loss: 6.2949 - accuracy: 0.0738 - val_loss: 6.9343 - val_accuracy: 0.0739\n",
            "Epoch 6/40\n",
            "196/196 [==============================] - 7s 38ms/step - loss: 6.1810 - accuracy: 0.0766 - val_loss: 6.9195 - val_accuracy: 0.0783\n",
            "Epoch 7/40\n",
            "196/196 [==============================] - 8s 39ms/step - loss: 6.0746 - accuracy: 0.0808 - val_loss: 6.9043 - val_accuracy: 0.0812\n",
            "Epoch 8/40\n",
            "196/196 [==============================] - 8s 40ms/step - loss: 5.9755 - accuracy: 0.0839 - val_loss: 6.8986 - val_accuracy: 0.0839\n",
            "Epoch 9/40\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 5.8819 - accuracy: 0.0869 - val_loss: 6.9255 - val_accuracy: 0.0856\n",
            "Epoch 10/40\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 5.7924 - accuracy: 0.0877 - val_loss: 6.9493 - val_accuracy: 0.0848\n",
            "Epoch 11/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 5.7055 - accuracy: 0.0878 - val_loss: 6.9705 - val_accuracy: 0.0850\n",
            "Epoch 12/40\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 5.6208 - accuracy: 0.0881 - val_loss: 7.0028 - val_accuracy: 0.0868\n",
            "Epoch 13/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 5.5404 - accuracy: 0.0898 - val_loss: 7.0256 - val_accuracy: 0.0877\n",
            "Epoch 14/40\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 5.4590 - accuracy: 0.0891 - val_loss: 7.0591 - val_accuracy: 0.0898\n",
            "Epoch 15/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 5.3810 - accuracy: 0.0900 - val_loss: 7.1095 - val_accuracy: 0.0883\n",
            "Epoch 16/40\n",
            "196/196 [==============================] - 8s 40ms/step - loss: 5.3016 - accuracy: 0.0916 - val_loss: 7.1312 - val_accuracy: 0.0892\n",
            "Epoch 17/40\n",
            "196/196 [==============================] - 8s 40ms/step - loss: 5.2256 - accuracy: 0.0921 - val_loss: 7.1739 - val_accuracy: 0.0883\n",
            "Epoch 18/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 5.1484 - accuracy: 0.0922 - val_loss: 7.2196 - val_accuracy: 0.0888\n",
            "Epoch 19/40\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 5.0736 - accuracy: 0.0938 - val_loss: 7.2656 - val_accuracy: 0.0898\n",
            "Epoch 20/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 4.9983 - accuracy: 0.0956 - val_loss: 7.3135 - val_accuracy: 0.0890\n",
            "Epoch 21/40\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 4.9267 - accuracy: 0.0958 - val_loss: 7.3261 - val_accuracy: 0.0909\n",
            "Epoch 22/40\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 4.8544 - accuracy: 0.0986 - val_loss: 7.3896 - val_accuracy: 0.0906\n",
            "Epoch 23/40\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 4.7824 - accuracy: 0.1005 - val_loss: 7.4214 - val_accuracy: 0.0895\n",
            "Epoch 24/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 4.7121 - accuracy: 0.1043 - val_loss: 7.4670 - val_accuracy: 0.0896\n",
            "Epoch 25/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 4.6442 - accuracy: 0.1084 - val_loss: 7.5086 - val_accuracy: 0.0896\n",
            "Epoch 26/40\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 4.5775 - accuracy: 0.1108 - val_loss: 7.5581 - val_accuracy: 0.0904\n",
            "Epoch 27/40\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 4.5128 - accuracy: 0.1158 - val_loss: 7.5987 - val_accuracy: 0.0902\n",
            "Epoch 28/40\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 4.4482 - accuracy: 0.1218 - val_loss: 7.6480 - val_accuracy: 0.0890\n",
            "Epoch 29/40\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 4.3852 - accuracy: 0.1287 - val_loss: 7.6955 - val_accuracy: 0.0898\n",
            "Epoch 30/40\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 4.3242 - accuracy: 0.1374 - val_loss: 7.7380 - val_accuracy: 0.0900\n",
            "Epoch 31/40\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 4.2659 - accuracy: 0.1458 - val_loss: 7.7813 - val_accuracy: 0.0890\n",
            "Epoch 32/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 4.2082 - accuracy: 0.1536 - val_loss: 7.8275 - val_accuracy: 0.0877\n",
            "Epoch 33/40\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 4.1515 - accuracy: 0.1654 - val_loss: 7.8754 - val_accuracy: 0.0892\n",
            "Epoch 34/40\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 4.0963 - accuracy: 0.1712 - val_loss: 7.9254 - val_accuracy: 0.0858\n",
            "Epoch 35/40\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 4.0445 - accuracy: 0.1805 - val_loss: 7.9734 - val_accuracy: 0.0866\n",
            "Epoch 36/40\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 3.9928 - accuracy: 0.1886 - val_loss: 8.0206 - val_accuracy: 0.0856\n",
            "Epoch 37/40\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 3.9444 - accuracy: 0.1962 - val_loss: 8.0684 - val_accuracy: 0.0854\n",
            "Epoch 38/40\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 3.8964 - accuracy: 0.2043 - val_loss: 8.1236 - val_accuracy: 0.0859\n",
            "Epoch 39/40\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 3.8498 - accuracy: 0.2104 - val_loss: 8.1720 - val_accuracy: 0.0864\n",
            "Epoch 40/40\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 3.8051 - accuracy: 0.2176 - val_loss: 8.2142 - val_accuracy: 0.0841\n"
          ]
        }
      ],
      "source": [
        "optimizer = 'adam'\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, y, validation_split=0.5, batch_size=128, epochs=40, shuffle=True).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-2.33350679e-01, -9.16510344e-01,  8.30077529e-01,\n",
              "         2.40142971e-01, -2.75902122e-01, -6.89862132e-01,\n",
              "         7.35970080e-01,  5.31997561e-01, -3.12776268e-01,\n",
              "        -2.84879625e-01,  8.72254431e-01, -9.31103826e-01,\n",
              "         1.36763096e-01, -6.07555330e-01, -1.05522335e-01,\n",
              "        -2.13132635e-01,  8.36365372e-02,  4.96200591e-01,\n",
              "         5.58740735e-01,  8.84089887e-01,  1.08871341e+00,\n",
              "         1.32499897e+00,  1.57897961e+00, -4.25619990e-01,\n",
              "         6.44715130e-01,  8.60921964e-02, -8.92276645e-01,\n",
              "         6.03485644e-01,  1.57727852e-01, -5.10180414e-01,\n",
              "        -2.17924580e-01,  3.55829820e-02],\n",
              "       [-2.35490538e-02, -2.57339627e-01,  2.54550606e-01,\n",
              "         1.41773850e-01, -1.18589461e-01, -1.91974729e-01,\n",
              "         2.02507287e-01,  1.86518282e-01, -5.24985157e-02,\n",
              "        -4.54624221e-02,  2.29012161e-01, -3.08183223e-01,\n",
              "        -5.59609607e-02, -1.67076021e-01, -8.88470113e-02,\n",
              "        -4.53293882e-02,  2.72324700e-02,  1.34184450e-01,\n",
              "         1.01811737e-01,  3.29145014e-01,  3.55572939e-01,\n",
              "         4.35603172e-01,  4.94358003e-01, -1.41462043e-01,\n",
              "         2.04332456e-01, -8.02420732e-03, -3.41441363e-01,\n",
              "         1.88399434e-01,  3.08237188e-02, -2.34657273e-01,\n",
              "        -1.10426709e-01,  3.87901776e-02],\n",
              "       [-3.14684846e-02, -8.35096017e-02,  8.64669085e-02,\n",
              "         4.62677553e-02, -3.01688891e-02, -2.50084642e-02,\n",
              "         7.20981210e-02,  3.00078895e-02, -4.17164061e-03,\n",
              "        -3.50821689e-02,  6.18910976e-02, -7.06119910e-02,\n",
              "         2.68229982e-03, -5.04604429e-02,  7.34871160e-03,\n",
              "         5.74861607e-03,  1.53980572e-02,  4.24129404e-02,\n",
              "         4.25065048e-02,  1.04961187e-01,  9.02101398e-02,\n",
              "         1.03206187e-01,  1.62869170e-01, -4.29459289e-02,\n",
              "         6.93196505e-02,  2.81672459e-02, -6.69212267e-02,\n",
              "         5.35023361e-02, -1.18747018e-02, -6.54454380e-02,\n",
              "        -9.84308124e-03, -1.49366795e-04],\n",
              "       [-3.02434683e-01, -8.22810113e-01,  7.95200348e-01,\n",
              "         9.27263945e-02, -3.98934096e-01, -3.53725642e-01,\n",
              "         1.12148011e+00,  1.11444545e+00, -1.17578059e-01,\n",
              "        -1.07387829e+00,  1.82029092e+00, -1.41457534e+00,\n",
              "         9.35117841e-01, -5.72441220e-01, -4.86891598e-01,\n",
              "        -3.68061930e-01,  4.08013910e-01, -3.17789495e-01,\n",
              "         9.22447681e-01,  1.12049210e+00,  9.77000058e-01,\n",
              "         1.08321583e+00,  1.41250002e+00, -7.90665150e-01,\n",
              "         1.11929417e+00,  2.30316281e-01, -3.77998382e-01,\n",
              "         6.82685375e-01, -5.87903485e-02, -2.19243929e-01,\n",
              "         3.78666580e-01,  4.68002349e-01],\n",
              "       [-3.73639259e-03, -1.08344898e-01,  5.15861213e-02,\n",
              "         6.42848462e-02, -3.51191163e-02, -3.39219719e-02,\n",
              "         2.15602964e-02,  6.75674453e-02, -4.78059286e-03,\n",
              "         2.03827508e-02,  8.14903006e-02, -1.02513410e-01,\n",
              "        -1.33161135e-02, -3.61088589e-02, -2.19755974e-02,\n",
              "        -1.74274798e-02, -1.96793899e-02,  4.53877933e-02,\n",
              "         5.56601910e-03,  1.18145376e-01,  9.29673687e-02,\n",
              "         1.51048914e-01,  1.72285900e-01, -7.14970604e-02,\n",
              "         4.84120399e-02, -1.73036810e-02, -1.11462303e-01,\n",
              "         6.12434819e-02, -4.88376105e-03, -7.38927424e-02,\n",
              "        -8.45958944e-04,  2.67054141e-02]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 6s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "probs = model.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3.51365181e-09 5.11457994e-11 3.16978177e-09 ... 1.24368675e-08\n",
            " 3.14890181e-09 4.78765187e-11]\n"
          ]
        }
      ],
      "source": [
        "print(probs[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUFHS4kHkyY"
      },
      "source": [
        "Para ver cómo evoluciona nuestro modelo del lenguaje, vamos a generar texto según va entrenando. Para ello, vamos a programar una función que, utilizando el modelo en su estado actual, genere texto, con la idea de ver cómo se va generando texto al entrenar cada epoch.\n",
        "\n",
        "En el código de abajo podemos ver una función auxiliar para obtener valores de una distribución multinomial. Esta función se usará para muestrear el siguiente carácter a utilizar según las probabilidades de la salida de softmax (en vez de tomar directamente el valor con la máxima probabilidad, obtenemos un valor aleatorio según la distribución de probabilidad dada por softmax, de modo que nuestros resultados serán más diversos, pero seguirán teniendo \"sentido\" ya que el modelo tenderá a seleccionar valores con más probabilidad).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LoGYpWOHd7Lr"
      },
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "def sample(probs, temperature=1.0):\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "    \n",
        "    probs = np.log(probs) / temperature\n",
        "    \n",
        "    exp_probs = np.exp(probs)\n",
        "    probs = exp_probs / np.sum(exp_probs)\n",
        "    \n",
        "    samples = np.random.multinomial(1, probs, 1)\n",
        "    return np.argmax(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fejfZldd4ou"
      },
      "source": [
        "Utilizando la función anterior y el modelo entrenado, vamos a añadir un callback a nuestro modelo para que, según vaya entrenando, veamos los valores que resultan de generar textos con distintas temperaturas al acabar cada epoch.\n",
        "\n",
        "Para ello, abajo tenéis disponible el callback *on_epoch_end*. Esta función elige una secuencia de texto al azar en el texto disponible en la variable\n",
        "text y genera textos de longitud *GENERATED_TEXT_LENGTH* según las temperaturas en *TEMPERATURES_TO_TRY*, utilizando para ello la función *generate_text*.\n",
        "\n",
        "Completa la función *generate_text* de modo que utilicemos el modelo y la función sample para generar texto.\n",
        "\n",
        "NOTA: Cuando hagas model.predict, es aconsejable usar verbose=0 como argumento para evitar que la función imprima valores de salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xOEZvnBXkODd"
      },
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "TEMPERATURES_TO_TRY = [0.2, 0.5, 1.0, 1.2]\n",
        "GENERATED_TEXT_LENGTH = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(seed_text, model, length, temperature=1):\n",
        "    generated = seed_text\n",
        "    \n",
        "    texto = seed_text.split()\n",
        "    \n",
        "    NUM_TEXTO = len(texto)\n",
        "\n",
        "    X_pred = np.zeros((NUM_TEXTO, SEQ_LENGTH, 32))\n",
        "    for k in range(length):\n",
        "        for a in range(NUM_TEXTO):\n",
        "            for w in range(5):\n",
        "                word = texto[a]\n",
        "                X[a, w] = modelWord2Vec.wv[word]\n",
        "        prediccion = model.predict(X_pred, batch_size = 32, verbose = 0)\n",
        "        generated += \" \" + reversed_word_index[sample(prediccion[0], temperature)]\n",
        "                \n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(palabras) - SEQ_LENGTH)\n",
        "  seed_text = \" \".join(palabras[start_pos:start_pos + SEQ_LENGTH])\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "\n",
        "    generated_text = generate_text(seed_text, model,\n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "    print()\n",
        "\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSMYZ2JdrSJg"
      },
      "source": [
        "Entrena ahora tu modelo. No te olvides de añadir *generation_callback* a la lista de callbacks utilizados en fit(). Ya que las métricas de clasificación no son tan críticas aquí (no nos importa tanto acertar el carácter exacto, sino obtener una distribución de probabilidad adecuada), no es necesario monitorizar la accuracy ni usar validation data, si bien puedes añadirlos para asegurarte de que todo está en orden.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3oT7pNvjrP2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.2\n",
            "Seed: nos despertaba a todos por\n",
            "Texto generado: nos despertaba a todos por que las que la dios la que que la a\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.5\n",
            "Seed: nos despertaba a todos por\n",
            "Texto generado: nos despertaba a todos por vez dios obra y ha de que un y algun\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.0\n",
            "Seed: nos despertaba a todos por\n",
            "Texto generado: nos despertaba a todos por aca toda un terrible un tras siendo corrillos matar balandra\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.2\n",
            "Seed: nos despertaba a todos por\n",
            "Texto generado: nos despertaba a todos por inesperadamente todo grave vale un juzgarme un gavias amargo todavia\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.2\n",
            "Seed: de los primeros barcos de\n",
            "Texto generado: de los primeros barcos de la la la la la que la la la la\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.5\n",
            "Seed: de los primeros barcos de\n",
            "Texto generado: de los primeros barcos de pues que que la que la la la su las\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.0\n",
            "Seed: de los primeros barcos de\n",
            "Texto generado: de los primeros barcos de describir nosotros nombraba la obra nota rumbo intentare extravagantisimos tropa\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.2\n",
            "Seed: de los primeros barcos de\n",
            "Texto generado: de los primeros barcos de derramar ruegos encargo fila hora llego y otros lo corria\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.2\n",
            "Seed: como un saludo de buen\n",
            "Texto generado: como un saludo de buen la la la la la la en la la la\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.5\n",
            "Seed: como un saludo de buen\n",
            "Texto generado: como un saludo de buen las la todas la la el la preciso y la\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 1.0\n",
            "Seed: como un saludo de buen\n",
            "Texto generado: como un saludo de buen ser poder olvidar a la dirigiendose a y tropa desarbolados\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 1.2\n",
            "Seed: como un saludo de buen\n",
            "Texto generado: como un saludo de buen tamaño polvos mano con moviendose pudiese trabajar eso quedo de\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.2\n",
            "Seed: con gravina otros fueron apresados\n",
            "Texto generado: con gravina otros fueron apresados la la los los la la la la la la\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.5\n",
            "Seed: con gravina otros fueron apresados\n",
            "Texto generado: con gravina otros fueron apresados y que la la sus que la la los la\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 1.0\n",
            "Seed: con gravina otros fueron apresados\n",
            "Texto generado: con gravina otros fueron apresados los vii muchos un lana sus cañones unos viendo representan\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 1.2\n",
            "Seed: con gravina otros fueron apresados\n",
            "Texto generado: con gravina otros fueron apresados preciso marineros lloroso maravillas advertir por la vale su peligro\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.2\n",
            "Seed: que lo fue de mi\n",
            "Texto generado: que lo fue de mi la un la la y la la la la la\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.5\n",
            "Seed: que lo fue de mi\n",
            "Texto generado: que lo fue de mi el cuando no verle a lo llegar algun parte usted\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 1.0\n",
            "Seed: que lo fue de mi\n",
            "Texto generado: que lo fue de mi pintar procesiones paseamos luchaban ayuda hombre trasbordabamos hueco fuera misericordia\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 1.2\n",
            "Seed: que lo fue de mi\n",
            "Texto generado: que lo fue de mi y un crei nautico estaban lagrimas salivazos doña abatido la\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.2\n",
            "Seed: y los naranjos de azahares\n",
            "Texto generado: y los naranjos de azahares la la la la la la la la la la\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.5\n",
            "Seed: y los naranjos de azahares\n",
            "Texto generado: y los naranjos de azahares lo esto eres el costa y cuanto la la la\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 1.0\n",
            "Seed: y los naranjos de azahares\n",
            "Texto generado: y los naranjos de azahares estas generacion papel habiendo poder verdad cifra mortal cola ciencia\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 1.2\n",
            "Seed: y los naranjos de azahares\n",
            "Texto generado: y los naranjos de azahares infelices asomados cuando volvio casacas ultimo caracoles mas ningun gemirian\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.2\n",
            "Seed: al capitan hardy se acabo\n",
            "Texto generado: al capitan hardy se acabo los la y la la la que la la la\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.5\n",
            "Seed: al capitan hardy se acabo\n",
            "Texto generado: al capitan hardy se acabo estado los en sus cortesia la de publica sabia la\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 1.0\n",
            "Seed: al capitan hardy se acabo\n",
            "Texto generado: al capitan hardy se acabo hasta mal señor vuelve ataque los los obscuridad orgullo como\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 1.2\n",
            "Seed: al capitan hardy se acabo\n",
            "Texto generado: al capitan hardy se acabo tablas aun señor que saben contraria severidad dejen adverti el\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.2\n",
            "Seed: a su lado iba marcial\n",
            "Texto generado: a su lado iba marcial la la la y la anque sabia la la la\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.5\n",
            "Seed: a su lado iba marcial\n",
            "Texto generado: a su lado iba marcial pronto salteadores su la amor preguntas francesa un ejemplo falta\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 1.0\n",
            "Seed: a su lado iba marcial\n",
            "Texto generado: a su lado iba marcial abandone los anque aspavientos comprendio colores tierra falta mostrando punto\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 1.2\n",
            "Seed: a su lado iba marcial\n",
            "Texto generado: a su lado iba marcial compañia frivolas eres finisterre inundar busca reventaria verbos chicos este\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.2\n",
            "Seed: buena distancia desde que avistamos\n",
            "Texto generado: buena distancia desde que avistamos sabia sabia sabia pudieran frivolas sabia sabia sabia sabia falta\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.5\n",
            "Seed: buena distancia desde que avistamos\n",
            "Texto generado: buena distancia desde que avistamos mismo salio mostrado mi falta sabia mostrado mostrado dejen sabia\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 1.0\n",
            "Seed: buena distancia desde que avistamos\n",
            "Texto generado: buena distancia desde que avistamos embarcarse trato compañeros malas pavimento solidas hablara aguas celebraron parecia\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 1.2\n",
            "Seed: buena distancia desde que avistamos\n",
            "Texto generado: buena distancia desde que avistamos comence inglesa asistido comprende haya librara vistos para aumentar deseoso\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.2\n",
            "Seed: retiraron con gravina otros fueron\n",
            "Texto generado: retiraron con gravina otros fueron sabia sabia sabia pudieran confesado sabia mostrado sabia sabia mostrado\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.5\n",
            "Seed: retiraron con gravina otros fueron\n",
            "Texto generado: retiraron con gravina otros fueron sabia falta sabia velera dejen descomunal sabia sabia salgan sabia\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 1.0\n",
            "Seed: retiraron con gravina otros fueron\n",
            "Texto generado: retiraron con gravina otros fueron adverti dejen severidad sabia sabia inglaterra estrecho mostrado sabia deseoso\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 1.2\n",
            "Seed: retiraron con gravina otros fueron\n",
            "Texto generado: retiraron con gravina otros fueron falta conjeturas artilleros sonante verdad hundio con gobierno hombre expresar\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "history=model.fit(X,y, validation_split=0.05, batch_size=128, epochs=10,shuffle=True,verbose=0,callbacks=[generation_callback]).history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBbmz9DMhVhc"
      },
      "source": [
        "## Entregable\n",
        "\n",
        "Completa los apartados anteriores para entrenar modelos del lenguaje que sean capaces de generar texto con cierto sentido. Comentar los resultados obtenidos y cómo el modelo va mejorando época a época. Comentar las diferencias apreciadas al utilizar diferentes valores de temperatura. Entregar al menos la salida de un entrenamiento completo con los textos generados época a época.\n",
        "\n",
        "El objetivo no es conseguir generar pasajes literarios con coherencia, sino obtener lenguaje que se asemeje en cierta manera a lo visto en el texto original y donde las palabras sean reconocibles como construcciones en castellano. Como ejemplo de lo que se puede conseguir, este es el resultado de generar texto después de 10 epochs y con temperature 0.2 usando El Quijote:\n",
        "\n",
        "\n",
        "```\n",
        "-----> Epoch: 10 - Generando texto con temperature 0.2\n",
        "Seed: o le cautivaron y rindieron el\n",
        "Texto generado: o le cautivaron y rindieron el caballero de la caballería de la mano de la caballería del cual se le dijo:\n",
        "\n",
        "-¿quién es el verdad de la caballería de la caballería de la caballería de la caballería de la caballería, y me ha de habían de la mano que el caballero de la mano de la caballería. y que no se le habían de la mano de la c\n",
        "\n",
        "```\n",
        "\n",
        "Asimismo, se proponen los siguientes aspectos opcionales para conseguir nota extra:\n",
        "\n",
        "*   Experimentar con los textos de teatro en verso de Calderón de la Barca (¿es capaz el modelo de aprender las estructuras del teatro en verso?) o con alguno de los otros textos disponibles. También se puede probar con textos de vuestra elección.\n",
        "*   Experimentar con distintos valores de SEQ_LENGTH.\n",
        "*   Experimentar con los hiperparámetros del modelo o probar otro tipo de modelos como GRUs o *stacked* RNNs (RNNs apiladas).\n",
        "*   Experimentar utilizando embeddings en vez de representaciones one-hot.\n",
        "*   (Difícil) Entrenar un modelo secuencia a secuencia en vez de secuencia a carácter.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
