{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# Laboratorio: Modelos del lenguaje con RNNs (Caracteres)\n",
        "\n",
        "[Enlace al cuaderno en el Github](https://github.com/aaronmed/7ro/blob/master/TAREA%208%20(23-24)%20RNN%20USANDO%20EMBEDDINGS%20(TRAFALGAR)/7RO_RNN_Caracteres_AaronMedinaMelian.ipynb)\n",
        "\n",
        "Aarón Medina Melián\n",
        "\n",
        "En este laboratorio, vamos a entrenar un modelo del lenguaje basado en caracteres con Recurrent Neural Networks. Asimismo, utilizaremos el modelo para generar texto. En particular, alimentaremos nuestro modelo con obras de la literatura clásica en castellano para obtener una red neuronal que sea capaz de \"escribir\" fragmentos literarios.\n",
        "\n",
        "Los entrenamientos en esta laboratorio para obtener un modelo de calidad podrían tomar cierto tiempo (5-10 minutos por epoch), por lo que se aconseja empezar a trabajar pronto. El uso de GPUs no ayuda tanto con LSTMs como con CNNs, por lo que si tenéis máquinas potentes en casa es posible que podáis entrenar más rápido o a la misma velocidad que en Colab. En todo caso, la potencia de Colab es más que suficiente para completar este laboratorio con éxito.\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Portada_Trafalgar_%281873%29.jpg/800px-Portada_Trafalgar_%281873%29.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consistirá en un archivo de texto con el contenido íntegro en castellano de Trafalgar, disponible de manera libre en la página de [Project Gutenberg](https://www.gutenberg.org). Asimismo, como apartado optativo en este laboratorio se pueden utilizar otras fuentes de texto. Aquí podéis descargar los datos a utilizar de El Quijote y un par de obras adicionales:\n",
        "\n",
        "[El ingenioso hidalgo Don Quijote de la Mancha (Miguel de Cervantes)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io)\n",
        "\n",
        "[Compilación de obras teatrales (Calderón de la Barca)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219433&authkey=AKvGD6DC3IRBqmc)\n",
        "\n",
        "[Trafalgar (Benito Pérez Galdós)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ)\n",
        "\n",
        "Como ya deberíamos de estar acostumbrados en problemas de Machine Learning, es importante echar un vistazo a los datos antes de empezar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## 1. Carga y procesado del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZNnzvXuqVVm"
      },
      "source": [
        "Primero, vamos a descargar el libro e inspeccionar los datos. El fichero a descargar es una versión en .txt del libro Trafalgar, a la cual se le han borrado introducciones, licencias y otras secciones para dejarlo con el contenido real de la novela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "import random\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D7tKOZ9BFfki",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "path = keras.utils.get_file(fname=\"trafalgar.txt\", origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGLvjLXrUUd"
      },
      "source": [
        "Una vez descargado, vamos a leer el contenido del fichero en una variable. Adicionalmente, convertiremos el contenido del texto a minúsculas para ponérselo un poco más fácil a nuestro modelo (de modo que todas las letras sean minúsculas y el modelo no necesite diferenciar entre minúsculas y mayúsculas).\n",
        "\n",
        "**1.1.** Leer todo el contenido del fichero en una única variable ***text*** y convertir el string a minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8WB6FejrrTu9",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "text = open(path, encoding=\"utf8\").read().lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkgGl8GWtUk8"
      },
      "source": [
        "Podemos comprobar ahora que efectivamente nuestra variable contiene el resultado deseado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hMFhe3COFwSD",
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del texto: 300039\n",
            "-i-\n",
            "\n",
            "se me permitirá que antes de referir el gran suceso de que fui testigo,\n",
            "diga algunas palabras sobre mi infancia, explicando por qué extraña\n",
            "manera me llevaron los azares de la vida a presenciar la terrible\n",
            "catástrofe de nuestra marina.\n",
            "\n",
            "al hablar de mi nacimiento, no imitaré a la mayor parte de\n"
          ]
        }
      ],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "print(\"Longitud del texto: {}\".format(len(text)))\n",
        "print(text[0:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "text = text[5:len(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n",
            "299920\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'fin de trafalgar\\n\\nmadrid, enero-febrero 1873.\\n\\nbenito pérez galdós; edición ilustrada por enrique y arturo mélida\\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fin = \"fin de trafalgar\"\n",
        "posicion = text.find(fin)\n",
        "\n",
        "\n",
        "print(len(fin))\n",
        "print(posicion)\n",
        "\n",
        "text[posicion:len(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "text = text[0:posicion+len(fin)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7TUXWiyvOj"
      },
      "source": [
        "## 2. Procesado de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x66_Vi_Gyxns"
      },
      "source": [
        "Una de las grandes ventajas de trabajar con modelos que utilizan caracteres en vez de palabras es que no necesitamos tokenizar el texto (partirlo palabra a palabra). Nuestro modelo funcionará directamente con los caracteres en el texto, incluyendo espacios, saltos de línea, etc.\n",
        "\n",
        "Antes de hacer nada, necesitamos procesar el texto en entradas y salidas compatibles con nuestro modelo. Como sabemos, un modelo del lenguaje con RNNs acepta una serie de caracteres y predice el siguiente carácter en la secuencia.\n",
        "\n",
        "* \"*Se me permitirá que antes de referir el gr*\" -> predicción: **a**\n",
        "* \"*e me permitirá que antes de referir el gra*\" -> predicción: **n**\n",
        "\n",
        "De modo que la entrada y la salida de nuestro modelo necesita ser algo parecido a este esquema. En este punto, podríamos usar dos formas de preparar los datos para nuestro modelo.\n",
        "\n",
        "1. **Secuencia a secuencia**. La entrada de nuestro modelo sería una secuencia y la salida sería esa secuencia trasladada un caracter a la derecha, de modo que en cada instante de tiempo la RNN tiene que predecir el carácter siguiente. Por ejemplo:\n",
        "\n",
        ">* *Input*:  Se me permitirá que antes de referir el gr\n",
        ">* *Output*: e me permitirá que antes de referir el gra\n",
        "\n",
        "2. **Secuencia a carácter**. En este variante, pasaríamos una secuencia de caracteres por nuestra RNN y, al llegar al final de la secuencia, predeciríamos el siguiente carácter.\n",
        "\n",
        ">* *Input*:  Se me permitirá que antes de referir el gr\n",
        ">* *Output*: a\n",
        "\n",
        "En este laboratorio, por simplicidad, vamos a utilizar la segunda variante (PERO USANDO PALABRAS NO CARACTERES).\n",
        "\n",
        "De este modo, a partir del texto, hemos de generar nuestro propio training data que consista en secuencias de PALABRAS con la siguiente PALABRA a predecir. Para estandarizar las cosas, utilizaremos secuencias de tamaño *SEQ_LENGTH* PALABRAS (un hiperparámetro que podemos elegir nosotros).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfJUIxW5m5C"
      },
      "source": [
        "#### 2.1. Obtención de las palabras y mapas de palabras\n",
        "\n",
        "Antes que nada, necesitamos saber qué palabras aparecen en el texto, ya que tendremos que diferenciarlos mediante un índice de 0 a *num_words* - 1 en el modelo. Obtener:\n",
        "\n",
        "\n",
        "1.   Número de caracteres únicos que aparecen en el texto.\n",
        "2.   Diccionario que asocia char a índice único entre 0 y *num_words* - 1. Por ejemplo, {'se': 0, 'me': 1, ...}\n",
        "3.   Diccionario reverso de índices a palabras: {0: 'se', 1: 'me', ...}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from unidecode import unidecode\n",
        "\n",
        "text = re.sub(r'[^A-Za-záéíóúüÁÉÍÓÚÜ\\s]', '', text)\n",
        "# Sustituir letras con tilde por su versión sin tilde\n",
        "text = unidecode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5bJ0NsbCbupF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Caracteres únicos: 28\n",
            "{'\\n': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
            "{0: '\\n', 1: ' ', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n"
          ]
        }
      ],
      "source": [
        "chars=sorted(list(set(text)))\n",
        "char_indices=dict((c,i) for i,c in enumerate(chars))\n",
        "indices_char=dict((i,c) for i,c in enumerate(chars))\n",
        "print('Caracteres únicos: {}'.format(len(chars)))\n",
        "print(char_indices)\n",
        "print(indices_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_B4AWo0ElwA"
      },
      "source": [
        "#### 2.2. Obtención de secuencias de entrada y palabra a predecir\n",
        "\n",
        "Ahora, vamos a obtener las secuencias de entrada en formato texto y las correspondientes palabras a predecir. Para ello, recorrer el texto completo leído anteriormente, obteniendo una secuencia de SEQ_LENGTH palabras y la siguiente palabra a predecir. Una vez hecho, desplazarse una palabra a la derecha y hacer lo mismo para obtener una nueva secuencia y predicción. Guardar las secuencias en una variable ***sequences*** y las palabras a predecir en una variable ***next_words***.\n",
        "\n",
        "Por ejemplo, si el texto fuera \"Don Quijote\" y SEQ_LENGTH fuese 2, tendríamos\n",
        "\n",
        "* *sequences* = [\"Don Quijote\", \"Quijote de\"]\n",
        "* *next_chars* = ['de', 'La']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NslxhnnDK6uA"
      },
      "outputs": [],
      "source": [
        "# Definimos el tamaño de las secuencias. Puedes dejar este valor por defecto.\n",
        "SEQ_LENGTH = 30\n",
        "step = 1\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0,len(text)-SEQ_LENGTH, step):\n",
        "  sequences.append(text[i:i+SEQ_LENGTH])\n",
        "  next_chars.append(text[i+SEQ_LENGTH])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplo: secuencia número 5:\n",
            "e permitira que antes de refer\n",
            "Siguiente caracter:\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "print('Ejemplo: secuencia número 5:')\n",
        "print(sequences[4])\n",
        "print('Siguiente caracter:')\n",
        "print(next_chars[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y3AmjYtHdLJ"
      },
      "source": [
        "Indicar el tamaño del training set que acabamos de generar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WVWqKxFcbwTu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "número de datos de training: 288600\n"
          ]
        }
      ],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "print('número de datos de training: {}'.format(len(sequences)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGQkKcwpLRJ"
      },
      "source": [
        "Como el libro es muy largo y tenemos muchas secuencias, podríamos encontrar problemas de memoria. Por ello, vamos a elegir un número máximo de ellas. Si estás corriendo esto localmente y tienes problemas de memoria, puedes reducir el tamaño aún más, pero ten cuidado porque, a menos datos, peor calidad del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2pm1Q19ppw8F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "288600\n"
          ]
        }
      ],
      "source": [
        "MAX_SEQUENCES = 500000\n",
        "\n",
        "perm = np.random.permutation(len(sequences))\n",
        "sequences, next_chars = np.array(sequences), np.array(next_chars)\n",
        "sequences, next_chars = sequences[perm], next_chars[perm]\n",
        "sequences, next_chars = list(sequences[:MAX_SEQUENCES]), list(next_chars[:MAX_SEQUENCES])\n",
        "\n",
        "print(len(sequences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FzgtAbPIs6f"
      },
      "source": [
        "#### 2.3. Obtención de input X y output y para el modelo\n",
        "\n",
        "Finalmente, a partir de los datos de entrenamiento que hemos generado vamos a crear los arrays de datos X e y que pasaremos a nuestro modelo.\n",
        "\n",
        "Para ello, vamos a utilizar *one-hot encoding* para nuestras palabras. Por ejemplo, si sólo tuviéramos 4 palabras (a, b, c, d), las representaciones serían: (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0) y (0, 0, 0, 1).\n",
        "\n",
        "De este modo, **X** tendrá shape *(num_sequences, seq_length, num_words)* e **y** tendrá shape *(num_sequences, num_words)*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zMBwZ9obNGNg"
      },
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "NUM_CHARS = len(chars)  # Tu número de caracteres distintos aquí\n",
        "NUM_SEQUENCES = len(sequences)\n",
        "X = np.zeros((NUM_SEQUENCES, SEQ_LENGTH, NUM_CHARS))\n",
        "y = np.zeros((NUM_SEQUENCES, NUM_CHARS))\n",
        "\n",
        "## TU CÓDIGO AQUÍ\n",
        "for i,sequence in enumerate(sequences): # i es el número de secuencia y sequence contiene los caracteres\n",
        "  for t,char in enumerate(sequence): # t recorre la longitud de la secuencia y char cada caracter de la secuencia\n",
        "    X[i,t,char_indices[char]]=1\n",
        "  y[i,char_indices[next_chars[i]]]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cisca pero cariosa y fraternal\n",
            "cisca pero cariosa y fraternal"
          ]
        }
      ],
      "source": [
        "print(sequences[0])\n",
        "for j in range(SEQ_LENGTH):\n",
        "  for k in range(NUM_CHARS):\n",
        "    if (X[0,j,k]==1):\n",
        "      print(indices_char[k],end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeUxz3HPm3l"
      },
      "source": [
        "## 3. Definición del modelo y entrenamiento\n",
        "\n",
        "Una vez tenemos ya todo preparado, es hora de definir el modelo. Define un modelo que utilice una **LSTM** con **128 unidades internas**. Si bien el modelo puede definirse de una manera más compleja, para empezar debería bastar con una LSTM más una capa Dense con el *softmax* que predice el siguiente caracter a producir. Adam puede ser una buena elección de optimizador.\n",
        "\n",
        "Una vez el modelo esté definido, entrénalo un poco para asegurarte de que la loss es decreciente. No es necesario guardar la salida de este entrenamiento en el entregable final, ya que vamos a hacer el entrenamiento más informativo en el siguiente punto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "vocab_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MSw2j0btYWZs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 128)               80384     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 28)                3612      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 83996 (328.11 KB)\n",
            "Trainable params: 83996 (328.11 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQ_LENGTH, vocab_size)))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1128/1128 [==============================] - 51s 44ms/step - loss: 2.3308 - accuracy: 0.2882 - val_loss: 2.1043 - val_accuracy: 0.3368\n",
            "Epoch 2/30\n",
            "1128/1128 [==============================] - 54s 48ms/step - loss: 2.0125 - accuracy: 0.3655 - val_loss: 1.9649 - val_accuracy: 0.3775\n",
            "Epoch 3/30\n",
            "1128/1128 [==============================] - 52s 46ms/step - loss: 1.9112 - accuracy: 0.3965 - val_loss: 1.8876 - val_accuracy: 0.4012\n",
            "Epoch 4/30\n",
            "1128/1128 [==============================] - 53s 47ms/step - loss: 1.8390 - accuracy: 0.4196 - val_loss: 1.8420 - val_accuracy: 0.4178\n",
            "Epoch 5/30\n",
            "1128/1128 [==============================] - 53s 47ms/step - loss: 1.7791 - accuracy: 0.4396 - val_loss: 1.7882 - val_accuracy: 0.4402\n",
            "Epoch 6/30\n",
            "1128/1128 [==============================] - 58s 51ms/step - loss: 1.7245 - accuracy: 0.4579 - val_loss: 1.7395 - val_accuracy: 0.4566\n",
            "Epoch 7/30\n",
            "1128/1128 [==============================] - 54s 48ms/step - loss: 1.6755 - accuracy: 0.4738 - val_loss: 1.7077 - val_accuracy: 0.4659\n",
            "Epoch 8/30\n",
            "1128/1128 [==============================] - 54s 48ms/step - loss: 1.6333 - accuracy: 0.4882 - val_loss: 1.6791 - val_accuracy: 0.4745\n",
            "Epoch 9/30\n",
            "1128/1128 [==============================] - 56s 49ms/step - loss: 1.5968 - accuracy: 0.5005 - val_loss: 1.6513 - val_accuracy: 0.4855\n",
            "Epoch 10/30\n",
            "1128/1128 [==============================] - 54s 48ms/step - loss: 1.5632 - accuracy: 0.5107 - val_loss: 1.6373 - val_accuracy: 0.4888\n",
            "Epoch 11/30\n",
            "1128/1128 [==============================] - 54s 48ms/step - loss: 1.5330 - accuracy: 0.5205 - val_loss: 1.6206 - val_accuracy: 0.4960\n",
            "Epoch 12/30\n",
            "1128/1128 [==============================] - 54s 48ms/step - loss: 1.5067 - accuracy: 0.5287 - val_loss: 1.6100 - val_accuracy: 0.5010\n",
            "Epoch 13/30\n",
            "1128/1128 [==============================] - 55s 49ms/step - loss: 1.4816 - accuracy: 0.5360 - val_loss: 1.5988 - val_accuracy: 0.5013\n",
            "Epoch 14/30\n",
            "1128/1128 [==============================] - 53s 47ms/step - loss: 1.4588 - accuracy: 0.5436 - val_loss: 1.5880 - val_accuracy: 0.5067\n",
            "Epoch 15/30\n",
            "1128/1128 [==============================] - 53s 47ms/step - loss: 1.4367 - accuracy: 0.5507 - val_loss: 1.5839 - val_accuracy: 0.5082\n",
            "Epoch 16/30\n",
            "1128/1128 [==============================] - 56s 49ms/step - loss: 1.4169 - accuracy: 0.5562 - val_loss: 1.5821 - val_accuracy: 0.5102\n",
            "Epoch 17/30\n",
            "1128/1128 [==============================] - 56s 50ms/step - loss: 1.3981 - accuracy: 0.5624 - val_loss: 1.5798 - val_accuracy: 0.5125\n",
            "Epoch 18/30\n",
            "1128/1128 [==============================] - 56s 49ms/step - loss: 1.3808 - accuracy: 0.5666 - val_loss: 1.5729 - val_accuracy: 0.5144\n",
            "Epoch 19/30\n",
            "1128/1128 [==============================] - 56s 49ms/step - loss: 1.3621 - accuracy: 0.5723 - val_loss: 1.5767 - val_accuracy: 0.5137\n",
            "Epoch 20/30\n",
            "1128/1128 [==============================] - 56s 49ms/step - loss: 1.3462 - accuracy: 0.5774 - val_loss: 1.5782 - val_accuracy: 0.5150\n",
            "Epoch 21/30\n",
            "1128/1128 [==============================] - 57s 50ms/step - loss: 1.3305 - accuracy: 0.5829 - val_loss: 1.5822 - val_accuracy: 0.5147\n",
            "Epoch 22/30\n",
            "1128/1128 [==============================] - 55s 49ms/step - loss: 1.3160 - accuracy: 0.5858 - val_loss: 1.5802 - val_accuracy: 0.5169\n",
            "Epoch 23/30\n",
            "1128/1128 [==============================] - 58s 51ms/step - loss: 1.3018 - accuracy: 0.5910 - val_loss: 1.5829 - val_accuracy: 0.5168\n",
            "Epoch 24/30\n",
            "1128/1128 [==============================] - 58s 51ms/step - loss: 1.2880 - accuracy: 0.5946 - val_loss: 1.5885 - val_accuracy: 0.5152\n",
            "Epoch 25/30\n",
            "1128/1128 [==============================] - 56s 50ms/step - loss: 1.2738 - accuracy: 0.5994 - val_loss: 1.5967 - val_accuracy: 0.5143\n",
            "Epoch 26/30\n",
            "1128/1128 [==============================] - 55s 49ms/step - loss: 1.2603 - accuracy: 0.6044 - val_loss: 1.6067 - val_accuracy: 0.5126\n",
            "Epoch 27/30\n",
            "1128/1128 [==============================] - 57s 50ms/step - loss: 1.2482 - accuracy: 0.6066 - val_loss: 1.6076 - val_accuracy: 0.5130\n",
            "Epoch 28/30\n",
            "1128/1128 [==============================] - 59s 52ms/step - loss: 1.2350 - accuracy: 0.6111 - val_loss: 1.6178 - val_accuracy: 0.5123\n",
            "Epoch 29/30\n",
            "1128/1128 [==============================] - 58s 51ms/step - loss: 1.2226 - accuracy: 0.6149 - val_loss: 1.6272 - val_accuracy: 0.5124\n",
            "Epoch 30/30\n",
            "1128/1128 [==============================] - 58s 51ms/step - loss: 1.2111 - accuracy: 0.6176 - val_loss: 1.6331 - val_accuracy: 0.5104\n"
          ]
        }
      ],
      "source": [
        "optimizer = 'adam'\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X, y, validation_split=0.5, batch_size=128, epochs=30, shuffle=True).history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib # Para salvar el modelo\n",
        "joblib.dump(model,'modelo_entrenado.pkl')\n",
        "model2=joblib.load('modelo_entrenado.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9019/9019 [==============================] - 38s 4ms/step - loss: 1.3989 - accuracy: 0.5735\n",
            "[1.3989105224609375, 0.5735412240028381]\n"
          ]
        }
      ],
      "source": [
        "results=model2.evaluate(X,y)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUFHS4kHkyY"
      },
      "source": [
        "Para ver cómo evoluciona nuestro modelo del lenguaje, vamos a generar texto según va entrenando. Para ello, vamos a programar una función que, utilizando el modelo en su estado actual, genere texto, con la idea de ver cómo se va generando texto al entrenar cada epoch.\n",
        "\n",
        "En el código de abajo podemos ver una función auxiliar para obtener valores de una distribución multinomial. Esta función se usará para muestrear el siguiente carácter a utilizar según las probabilidades de la salida de softmax (en vez de tomar directamente el valor con la máxima probabilidad, obtenemos un valor aleatorio según la distribución de probabilidad dada por softmax, de modo que nuestros resultados serán más diversos, pero seguirán teniendo \"sentido\" ya que el modelo tenderá a seleccionar valores con más probabilidad).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LoGYpWOHd7Lr"
      },
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "def sample(probs, temperature=1.0):\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "    \n",
        "    probs = np.log(probs) / temperature\n",
        "    \n",
        "    exp_probs = np.exp(probs)\n",
        "    probs = exp_probs / np.sum(exp_probs)\n",
        "    \n",
        "    samples = np.random.multinomial(1, probs, 1)\n",
        "    return np.argmax(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fejfZldd4ou"
      },
      "source": [
        "Utilizando la función anterior y el modelo entrenado, vamos a añadir un callback a nuestro modelo para que, según vaya entrenando, veamos los valores que resultan de generar textos con distintas temperaturas al acabar cada epoch.\n",
        "\n",
        "Para ello, abajo tenéis disponible el callback *on_epoch_end*. Esta función elige una secuencia de texto al azar en el texto disponible en la variable\n",
        "text y genera textos de longitud *GENERATED_TEXT_LENGTH* según las temperaturas en *TEMPERATURES_TO_TRY*, utilizando para ello la función *generate_text*.\n",
        "\n",
        "Completa la función *generate_text* de modo que utilicemos el modelo y la función sample para generar texto.\n",
        "\n",
        "NOTA: Cuando hagas model.predict, es aconsejable usar verbose=0 como argumento para evitar que la función imprima valores de salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed: ndalucia en tal estado\n",
            "que tod\n",
            "\n",
            "o"
          ]
        }
      ],
      "source": [
        "#CÓDIGO Ejemplo: Predice el siguiente caracter de una secuencia aleatoria\n",
        "# Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "# a partir de ella\n",
        "start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "print(\"Seed: {}\".format(seed_text))\n",
        "print()\n",
        "#----------------------------------\n",
        "# Aquí representomos la secuencia seed_text como codificación one-hot\n",
        "X_pred = np.zeros((1, SEQ_LENGTH, NUM_CHARS))\n",
        "# Construimos X_pred a partir de seed_text\n",
        "for t,char in enumerate(seed_text): # t recorre la longitud de la secuencia y char cada caracter de la secuencia\n",
        "  X_pred[0,t,char_indices[char]]=1\n",
        "#-------------------------------------\n",
        "#PREDICCIÓN\n",
        "prediccion=model.predict(X_pred, batch_size=32, verbose=0)\n",
        "print(indices_char[np.argmax(prediccion)],end='') # Muestra el caracter más probable para la sequencia 100\n",
        "##################################################333"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ndalucia en tal estado\n",
            "que tod\n",
            "dalucia en tal estado\n",
            "que to\n",
            "dalucia en tal estado\n",
            "que too\n"
          ]
        }
      ],
      "source": [
        "print(seed_text)\n",
        "seed_text=seed_text[1:SEQ_LENGTH-1]\n",
        "print(seed_text)\n",
        "seed_text+=indices_char[np.argmax(prediccion)]\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def genera_texto(texto, model, length):\n",
        "  X_pred = np.zeros((1, SEQ_LENGTH, NUM_CHARS)) # Declaramos X_pred\n",
        "  for k in range(length): # Genera el número de caracteres fijados en el parámetro length\n",
        "    #Inicializamos X_pred a cero\n",
        "    for a in range(SEQ_LENGTH):\n",
        "      for b in range(NUM_CHARS):\n",
        "         X_pred[0,a,b]=0\n",
        "    # Aquí representomos la secuencia seed_text como codificación one-hot\n",
        "    # Construimos X_pred a partir de seed_text\n",
        "    for t,char in enumerate(texto): # t recorre la longitud de la secuencia y char cada caracter de la secuencia\n",
        "      X_pred[0,t,char_indices[char]]=1\n",
        "    #-------------------------------------\n",
        "    #PREDICCIÓN\n",
        "    prediccion=model.predict(X_pred, batch_size=32, verbose=0)\n",
        "    print(indices_char[np.argmax(prediccion)],end='') # Muestra el caracter más probable para la sequencia dada\n",
        "    #Actualizamos la cadena de entrada\n",
        "    texto=texto[1:SEQ_LENGTH]\n",
        "    texto+=indices_char[np.argmax(prediccion)]\n",
        "    #texto+=indices_char[sample(prediccion[0],1)]\n",
        "    #print(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed: erto estrechaba con su helada \n",
            "Texto generado:\n",
            "de la escuadra de los marineros se almirante de los mas combatientes la escuadra completi con el combate de su andaner c"
          ]
        }
      ],
      "source": [
        "# Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "# a partir de ella\n",
        "start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "print('Seed:',seed_text)\n",
        "print('Texto generado:')\n",
        "genera_texto(seed_text, model2, 120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xOEZvnBXkODd"
      },
      "outputs": [],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "TEMPERATURES_TO_TRY = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "GENERATED_TEXT_LENGTH = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(seed_text, model, length, temperatura=1):\n",
        "    generated = seed_text\n",
        "    ### TU CÓDIGO AQUÍ\n",
        "    ######################################################################\n",
        "    texto=seed_text\n",
        "    X_pred = np.zeros((1, SEQ_LENGTH, NUM_CHARS)) # Declaramos X_pred\n",
        "    for k in range(length): # Genera el número de caracteres fijados en el parámetro length\n",
        "      #Inicializamos X_pred a cero\n",
        "      for a in range(SEQ_LENGTH):\n",
        "        for b in range(NUM_CHARS):\n",
        "          X_pred[0,a,b]=0\n",
        "      # Aquí representomos la secuencia seed_text como codificación one-hot\n",
        "      # Construimos X_pred a partir de seed_text\n",
        "      for t,char in enumerate(texto): # t recorre la longitud de la secuencia y char cada caracter de la secuencia\n",
        "        X_pred[0,t,char_indices[char]]=1\n",
        "      #-------------------------------------\n",
        "      #PREDICCIÓN\n",
        "      prediccion=model.predict(X_pred, batch_size=32, verbose=0)\n",
        "      # print(indices_char[np.argmax(prediccion)],end='') # Muestra el caracter más probable para la sequencia dada\n",
        "      #Actualizamos la cadena de entrada\n",
        "      texto=texto[1:SEQ_LENGTH]\n",
        "      #texto+=indices_char[np.argmax(prediccion)]\n",
        "      texto+=indices_char[sample(prediccion[0],temperatura)]\n",
        "      # generated+=indices_char[np.argmax(prediccion)]\n",
        "      generated+=indices_char[sample(prediccion[0],temperatura)]\n",
        "    ######################################################################\n",
        "    ### FIN DE TU CÓDIGO\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:  trabuco segun mis ideas con e\n",
            "Texto generado:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' trabuco segun mis ideas con es mar de so que tabia de los meores de los nartes de la cardera yorque se aconta de la escuadra completinte que en el plcazar de su cabeza y da conversacion ye la cabpaada confestabon en la costaa en la cabdera d el combate de no se valvio a la escuadra de los marineros se almirante de luestra conte'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "# a partir de ella\n",
        "start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "print('Seed:',seed_text)\n",
        "print('Texto generado:')\n",
        "generate_text(seed_text, model2, GENERATED_TEXT_LENGTH,temperatura=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "\n",
        "    generated_text = generate_text(seed_text, model2,\n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "    print()\n",
        "\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSMYZ2JdrSJg"
      },
      "source": [
        "Entrena ahora tu modelo. No te olvides de añadir *generation_callback* a la lista de callbacks utilizados en fit(). Ya que las métricas de clasificación no son tan críticas aquí (no nos importa tanto acertar el carácter exacto, sino obtener una distribución de probabilidad adecuada), no es necesario monitorizar la accuracy ni usar validation data, si bien puedes añadirlos para asegurarte de que todo está en orden.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3oT7pNvjrP2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.05\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y de la costa no se hesarandona la mscuadra ce la cabeza y como los mropositos de la costa no se desabandon de la costa de la contendiencia a lo carina y la costa de la contercia a los mngleses y los marineros y se representaba en la cabeza y como los mromeros se habian sido de la costa do se heceo es\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.1\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y de la cosa de la costa ne la conta do se da de esta esta pompanada y desla casta no se depirado de la costa do se dece a ma marina y la costa de la costa ne la conta do se hecigiando la casla y la maro de la cobeza y como los parineros entaban en el rar a la cscuadra domo si el repto de la conta ne \n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.2\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y ca mabeza y cas pasines de la ciroo completa lasducirior de la costa ne la cscuadra como si el repto de la cobeza y como u la carina y mo se aijo esta ee ri la armirar a los nngleses yue se helciaban d lo cscuadra como si el resoesentaba en la cabeza y aono se el resto due enta es el sar ae ho que d\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.3\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y dltla caba eo monversacion ye la mosta n cos has retierdo en ml dar mugurana que sn la caperio de la mectees de clla yu repiradal man y lu aasaza qlla mscuadra p posloestro  maraceones ae li ca tica de la cala de la casfendie a los hivios dn la cuavdesa de  peca da cosara e cor ul despico qe   cicho\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.4\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y daltporo la hscuadra don la cmaunido q suscaor das dess mon tanto saento a lesar de habia eosoz  ee la valta yi de eichende cue ns ca cscuadra ce laspobe ye alrijaron a las car romderia esto combiti dl\n",
            "aarine y aa mala de lu resimucna y ao pesada qe pe quedd sas oropositos me sa cccdia al cesuerdo q\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.5\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y ce ua fadar  desoino de lueca  su espa ee ri eurando ylso midtoria dlnu hmuq aer lar qardido en mudoz ear ln pavio dellas mjos y en eanlo  escrnra ce lunsirver aa banga fon la tccuricon mn dsa tinchal ye lo pscuadra yam srandes aero qn martido mir somo si el sesocte eue eo he eeeilra ea  aaerzas dur\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.6\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y nelau aentr aocde ae lievtro  maros eebia sino ibgles bo he hnoae r e nul o vue yourruca a sisso eutta yuto ei tnteue mstacs syele eu io que de hoseonservaba qllu  cocqise com mu pi aerarondesmpre pora mas daqcas dos has rea qosconboranion de\n",
            "mioulla que da hcbenar aiy mstasa  ebcucir ee la cgttgtra\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.7\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y dan las par pebia  se os de iticial dan\n",
            "tis mspudlae ao  galdesas\n",
            "faso us rosuo de luarradir ga  iromoros m li  ias ridos le tici e las qibiasy ellas aivs yegmuegon gados ea cis ilos daber la mnprico ion de aartentceo ae muevtrasbomsideraba qbcsombrende qs ea eida q cl pgferis\n",
            "d urndtaar lambre stnc\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.8\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y machos dnisao  heroionon il nquel a fbusion d nom ei pra el ea vepha sera qu uldian aomosaue ie duerae s te ado mranina isbo vsalama do nx ma cagurotia ne qo oeodrno dl maente\n",
            "neli dascial y euraieraa\n",
            "momrajlgnce covtai  ransugtia sl lascaata ne due uue o li qalido s nesea poche pora onte c nqia tu \n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.9\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y dllvuaa m eorrz pe mue fo tajo o  pe ie eelida yulid de geoislacarae lntfunit duzmmue lltcl aoegor loestro neroona er bmenas yesaido minre aas cesad\n",
            "comdeacoesencamanos ln uiscado sl de i ai gunapacie ain\n",
            " paenncermn casesberrcion ye tinsadeorqli iosrta ne  nnseea e meroabae eue yiniaue co sicin b\n",
            " \n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.0\n",
            "Seed: or prudentes eso es lo digo y \n",
            "Texto generado: or prudentes eso es lo digo y ea  muerrstaue ne lonociaron al a aonmocun a  eolu qs cusirdr qrro ei sspasaarooreo o ieltudidar   nl\n",
            "eontsnisea yspircaandna se halre\n",
            "dusiial c\n",
            "vq unos ralub s heriont  doeoeron punaosha oro metmavadle te eil drrdo erra podvm a dar ue eabnaroso panfcs os vntutcicdose  a n ue nar uoco yi\n",
            "ea mraniasa\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.05\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que el castio el combate de la conversacion de la colla de la coreza y en el promento de la drsabde la calle de la canversacion de la calle de la calle de la canversacion de la calle de la canversacion de la calle de la cobara el perdono de la escuadra domprendi que ll crlado de la pola de la eosa de la\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.1\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que el mar en la casa dn ca canta ne la cola de la posa de la eonversacion de la easa de la conversacion de la casa de la canversacion de la dscuadra comorendi que la escuadra complatante en ea conta do se entontraron de la ersa a d la canversacion y ll mar en la casa de la escuadra de la calea de  car \n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.2\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que ll bombate de la prsabo de la oespues de la conversacion de la dala ee la cscuadra ce la despsuerte de la pidta de la canversacion de la panversacion de la mscuadra conbinada yn casio en el mea eue ll celsamiento de la valabde la casle de la mara dl celado de la primera de la eespues de llguno  dons\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.3\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que el ea caspna della sgtirli c eono la ceninaria y ca  danos y la conversacion ye la cantersacion d de li amota cor la cosversacion de la crsabde la cala dn lltascontuderadion de la eida de la pano te lo caleza para el pensamle la eesa de la mespsuoras ses crandes ingelins tristes le nl eenigro ye la \n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.4\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que sa emlesia artlesacero ei amo de la cidta della casa de la pida dnta a te la cara  carte de la mricialidad e co moeptelione la vable de la eelaga y el alcazar el ll peimer clmcomoza de lo presencia ce lu aaber de la einee ce lu  aasos yue ne he paee que la excuadra domblendera la caleta dre a conole\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.5\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que men dratdui l a  una ee la poriia a en lisamo ea panila lellascisa\n",
            "eos con pumas en el duoto de qstrsde laceo cu aablara eue sa eunea en prrdonase la monvirsacion euecano p li cmo qon ua \n",
            "cosaistasan eicndose mntre dascesir\n",
            "a eadas titpirmano yntonces nue ns el ornto de las deoundos de lu par qntigi\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.6\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que ee vij ellas mot srosantiardo eanteodos tos ponvemtouas q dn mosmo cscuamo nu alo e tes maandunas cn ca cscuadra pe ta grenaado aambre drosero ye li corp cqnue se ea ia  eido darsanio aen la caseza ee la\n",
            " melem pe lo sobii no  dieio en eejos don sl damt de la detabde  ruque\n",
            "cdu se qcggrarp aadiz \n",
            "en\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.7\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que la to dauia de pues de mlaneara eodos\n",
            "drslas baazos ya lara d  cubia la iesillar ee leraela y lu ea iontcaua eeslus vismusoon\n",
            "s dono mu vesabanoia o le\n",
            "pas cartros no babia eindomo e ne octdencia daaste a pa cnclicillo si siste ma  beefuvzgas due tqlpembate enpsvari eonttasdlcuna d lur doftfir gl an\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.8\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que ls caarpa nl re la yu \n",
            "u odtestabo err rl vas smmto mnddna losfersacion eue lo eibir d eextda  uarronas c lu cseeriaddo car la musva te mltoca\n",
            "os a uidrpo errs la  eares cinai tiltinaei pasozlo\n",
            "afhbctlertodo loseue mania aa aaaierte yltla betary dianeddole en bosico cuncentabde sa tablara \n",
            "oc tiiste\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.9\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que do fendasan dur ulecscazorain ymtrba mntla sscuadratcadasluy\n",
            "malarabeda c  se hacir\n",
            "aldesdo mn fn a ce cbri iabpoea an ti midodie peo hacla peldidi soro ae\n",
            "aspuracios s yn lamla pelia io  p co cntoea da ve as\n",
            " arro eajiodllerca  aoppeeraueseen s drienlas qon pmeivta tca rreo pl\n",
            "bielo eo  tiidsmoncas\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 1.0\n",
            "Seed:  jose maria se establecio que \n",
            "Texto generado:  jose maria se establecio que efrayacle\n",
            "mascueer a \n",
            "ur\n",
            "a  qe pcirte deboas aasce niloadasu lqaepl tistncib n\n",
            "eusinlabia cuedod  enodablto o dn eu \n",
            "due cl brnigro ye  reontimob an mamtenion dea maspagen fe parcanpo alelrix opa eeneelbb y  sano  qealaltjsir fn aseaebo graneeuji d ponosci anor\n",
            "an poeoivn inechhees a  de ticeta  eom\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.05\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo de la conversacion y la cobeza y lon el palecio de la conversacion y la conversacion y la conversacion y la monversacion y la conversacion y la conversacion y la cano para el aan juan y como un santisima trinidad d en el pombate de la canversacion y la conversacion y la conversacion y la conversa\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.1\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo de la conversacion y lon el pareci que la cscuadra completa la comara dl cerdono de la conversacion y la cano para el aan juan o que sa conversacion y la cobt ee la casa d la conversacion y la contersacion ye la casara cl vardono de la cosa d la conversacion y ca contenta con la casa a la convers\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.2\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo de lo conta de la amo en el aronomito de la pasara el posco de lu mmb esta consentimn ll manta ana de la canversacion y la vanversacion y l  alcazar sa coba de la eanversacion y la caninaria ye sa eonversacion y ca cabeza y pon ll postoo de la comara en pesdono ds ll aronto de la coma ee la coba \n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.3\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo de la canversacion de la cola a la aida pe su malidi ded  de los caones sue poracian ln ma dora ee la canversacion y c lo cida de sa pasara en ea casta de mo conversacion y la canversacion y cue lon mandos y ln ea bscuadra dompledo yue las pntis le la carte de lropto d dor clla mida\n",
            "dor ennas ee \n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.4\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo ye las daondos de lu eromositione de ma vidta de sue ee lardoe  la dscuadra tom eneaacion yara ao mabge ae ea contersacion y ca pntarici dl celigro peslu madido d n  aanta ana ya caseza s lor un aolvero d eo vecia eenopmamonte dlla canveriacion ye lu cestor a ne la vanta de airiial f cue la donte\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.5\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo de lo canaalabl srisunda estoav en soal dstqirlioe ear ln conazon qacia due so lalir cedo dpvadiz pesto en ln noomero qalespina yl la caixelc tear parcos d al alcazar senira  sa erima de mn gonos due le tbrecia ana la\n",
            "patta pe lluilla cscuadra cs cindabde li cuei  ia ye sa erizazs\n",
            "ne pispsaindzo \n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.6\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo qe lquella rscoad\n",
            "dende intayaa \n",
            "cldenarde s uella oon torsedo tn susebmes somaeae d tasvenera cara mo ms aar n nosi lrnrino di ao berea don lue dusdara cas draste dxiueiende qlla cista de pna lscheta \n",
            "ln\n",
            "luvio crn eneo\n",
            "que ls norda qe vicionde larespina yan pomo usta ne vispura  n an equelltombn\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.7\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo yue fa  eibta  larecis lam uaeos l da  pebial s pus pimpomtenceseaibio es nl ba ascuevr qor lu gpre ai sas tasespines pegcabian sondirmodle iue aro eor ln sodento alplin \n",
            "pcodeel rabia el dglazen uo caueel  conovos cojos de cnhirti dl ray ynplrpand ye resbrrmabdo ce lu  uasbsntos q so sarino pe p\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.8\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo drr lltaste retpliondaaanasa li nue nl\n",
            "a  ono \n",
            "al qebea ontsispararlora rntu\n",
            "tpasprapult yral ter eltari dll mquello loerte que sa bncuadra d dephran enoajicialeya aijo aesenpina es onces vue bor lo dabmo a lo puricda d jor\n",
            "untimo me i e lllitincie qel niaa loanudndo sue lelia ma perece tue has p\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.9\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo \n",
            "vlcatasdo smas gos aonoiados rue clue eoea \n",
            "qoscn oa labian d das qamusdenees casbn b eovviron ho hiqriedica ce sisearuudeu os dna yrdbaen e\n",
            "tesee henve  pecantpe punda o ml hn  v te apstanl de tico  sa  mnenas te ro pscuadra iioanvrmgr  codosll aaliandomesp ll ro de praealaames lescique eoloire\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 1.0\n",
            "Seed: justo el san\n",
            "francisco y el ra\n",
            "Texto generado: justo el san\n",
            "francisco y el rayo resrigo ed   suyo lessuscieilla rnoaetaa ooscue\n",
            "la negea ye yoeomo ne\n",
            "dx undatto\n",
            "yelao nrtir esq cheioso c svntodudo\n",
            "ealre uo parana mui bevta hmure\n",
            " nib\n",
            "reistda lc tudio d chmosbedpi taursuccidye breoleorellerir\n",
            "qe vues o yl mapeei\n",
            "ds ien jencinor  pedo\n",
            "en se loseonomaue lid e cimos eos dajcosin\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.05\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un buque de la camara de ma contersacion de la camara el cario y el el mar no se plegrar a la vida de sa camara en ea cabeza y con el pecece que na contenta aon la camara el cario y el carino de la cantersacion de la camara el cario y en camino de la camara en la cabeza y con el perece que ne habia con\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.1\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un panta ana de la casa de la canventa ce la camara el cario y el camino de la camara en cario y el el crimer calocidad el mariyabia cantra dl hombate dellu aajo de la cara de la casventa con la contenta a la cora el corio y en el mar no se alegrar a la varara de ma casa de la cantersacion de ma camara\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.2\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un surco de mi amo eue sesla casa de la cantersacion de la cobara en la caba en el arma de la comeza p eon el deligro de la cantenta don las mngleses a las drecositos de la camara ee la contersacion de la cameza y con elta ae retiraro de mi amo qstabconteariarde la canta no habia consedia de la casta n\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.3\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un pantosima trinidad y cn masino e l  almazar de ma canvesta aon la cosplacion de li amo ea caror parte de la cida d ml rarinesoloco de si emo eon ea li amo es el plmarante celma cosa de la conventa de lu msta caracia da conta de ladia y llgaaro de rborcaba ql la coba ee ladi el eir adero de li amo hu\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.4\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un santo ana de la pida ae su mmiiemieo de sa amo es ul somtoco dn ml ceimciperei de aircial y aom lomontia de sadaas yer ea canta eo hodde de la\n",
            "come ee lu pmonecorria con ua csplti se lnasaritue ma casteaio ae la cmo q sosqas remona de las carines de ea ergula de lue san aadconpaemde c un lanto ana d\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.5\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un sereeble\n",
            "da ll mecia ta cad ml pambate do se cue a ehgparoa do lasvscuanad de la caricia yn el mlmazar pe ca canta te lu  misposicacas\n",
            "ye sadespina iarque ea dscuadra cios yoes pa sortiendi due la aonversacion de le amo hn ea cirdad e de ne habiancl rnda de su airmda ee oa cobeza y aaego leolrosento\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.6\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un jormoso due hi ha des \n",
            "rgunas fntaranas snandonados con uo  noaderlas que no vientn en la lurae a yr rncimi mantll cumaate de nance aobia oorieco a l andira a tos daen en la vaseza p le he mue nstoba de  pombate dablaren el soso\n",
            "pa\n",
            "nmo e eo psmdo ee lrn muestra vrretaanion dom ra el ciroad a de la  \n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.7\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un iamtcermccado d sa vala yon li cirino c nque a ema  aue ea cenpue do donpn\n",
            "mos nrreles ae msconaba  sa  pivan \n",
            "aeman tos cnturnas \n",
            "eliguaar da\n",
            "bilda d cl nasto dn auberl do ma\n",
            "que tn\n",
            "hntecielmo qieroe yue no macdasque aa nonteal aorque sltura rmgnrama eomtra do\n",
            "sura  oleea cerria l aecva caerte teci\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.8\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un haerpo de lispaeton cnflesatssbgeanpecde uqrosando a dnaioprtieble ymaosibre\n",
            "qorrem de cal do pna cunea domp sencisie slue hejonlpe ta eira p qospues de\n",
            "la iej sorienteoscomereta ee ai mraierta sodosma cebrreae aijianocto\n",
            "qe\n",
            " crinided elta sd  ei earmicluecue da garermcilga di clsol ta y p  e qll ma\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.9\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un vasaaemde cui\n",
            "yadosmn sbuella \n",
            "scubce qltoe  s galague que yl deco tucee m eodiroporinu eerlontuve t dn ca paleldo    qe  dudaneoadlas pare oodema\n",
            "luscuspungo hado noyho melde el ql biltues de aa lapeta \n",
            "ai aue ne  pmbargaruna dela donsiaterio  que orso  mortis aesle\n",
            "idad\n",
            "ls curco ne \n",
            "bezvorrdhegvec\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 1.0\n",
            "Seed: y fina de\n",
            "perfecta forma y un \n",
            "Texto generado: y fina de\n",
            "perfecta forma y un eaner sobas ya prseve n ee eoeegi\n",
            " eoiioro eclui  noas duosead n rus airastes mecilrai ls\n",
            "aanefmugon lr dodo  sotmdictinaoscs brbo\n",
            "to ieaa\n",
            "pijor ponp silronerrdo\n",
            "e l arque sa laz\n",
            "iq  da cirmdn sue bl caeialaer deeamaden rsue eo  so hibeue sa cmpuna sa cevo osae la selna iai serudsd oilsabta due co  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.05\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestra anteria de la catria de mi amo en ea casa en cl combate de la patria de mi amo en el alcazar de la camara de la camara de la camara de la camara de la camara de la camara de la camara de la camara de la camara de la camara de la camara de la camara de la camara de la camara de la camara de la cam\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.1\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestra antha de la camara de la camara de la pamara de la camara de la camara de la camara de la catria de mi amo en el plcazar de la carara de li amo en comaate de la casara de la camara de la camara de la pamara de la camara de la camara de la camara de la catria de mi amo en ll plcazar de la cama ds \n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.2\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestra aisma tara ci amo er ea coseza dara qs sspiciemo me la cetaia de mi amo en cespocho de la mamara de lu amonga de la camara de las palos de la pumara de la amo el celsabiento de la varina d losque me parecea a  unas dasaban de la camara de ma cata de au aabia comfuamada qe ma pinea de ma amo es aa\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.3\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestra altanada yor ll combate de auandcali ye la pasara de li emo es eespaes de poderlo cono se cabia eontuedto aecunrioa almas cngleses qomp este esta pontundboso comtentr da eamaa de mi amo en combate de la cida dn la casa en cl plcazar de ma crlria de li mmo en cosco de lu aromera de sue se parecia \n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.4\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestra monteamia d eo te mod ll pombate desca pscuadra domprega yn cas yo maistezpc alcazar y com esta de aa paopudia delmu ambgo yara dl pas yi anderes de csto apoebto aon la pscuadra dscaolde montesta en ea oasa rl cauin\n",
            "de mi amo eue eabia cntrezaaddon la cosversacion y eom du cosn   ce li aortado qo\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.5\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestra pnade  a e qravena isteciel e cu coendo frn ee os de astectalrdo de\n",
            "lu cepecie p eas cavios es la trmvience de quda sal entences vecpues de ln sumbaero de mquel as iubases y com esplasoa la escuadra dellruella mscuadra deiste no pe que pe eandose lara di amo cue espa drca eostunaia pento qe niss \n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.6\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestra drinllacion co oonandealonso poando au remida ya piser  dn cereo y staati cs\n",
            "aavio qe mo pierra ceslosci lsbaora cue le hpaedaabmo la raea c qa  brcoros d ln eaai oanelirta s li\n",
            "auno ia raandfel eoseao ae lstr re csto aecoso d con el burco de\n",
            "mi amo ea ixtienta de do ce vsun francesca con sa maza\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.7\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestrascicga qlta sabia comoatioo res maso sn avi donteamml merrianie cluella aar sania em aue a combien coree ye lquello tl c daando pntmadgitas entas rlmfiresses parscisque sacia eodaend la paplesenta don ma pidta do cijia eue nomteanee en ea  majderas des aoaos plegs como mom eneo es je ln  mrnera en\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.8\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestrosfxcoadra d ea \n",
            "mavios\n",
            "uelianen ee cems os noseudaestaeoa auento sm uduel a dunha arpecifellconoooo r mordo yes ll enbue da qoeam liti  il emaa delmu moltamil mi\n",
            "emo ztar mlteadra sar hllunosyentilar ye mes eimazon se\n",
            "pu aesldi arnosi t artul r mon aa ll ama marosezpe msposmoc ain\n",
            "que dos le ia de\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.9\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestra\n",
            "miint\n",
            "do pe pcgi pabea liqrande puaio con ro megvonaoa eue adglebantea bxcuadra brigatsn lugur loce acse mue decotcad sl cafo eoasdo harefo deboreo eornro pedzas l oobia os i sqauriento e euraecque hlta npuillassmaeru tunp annapaa \n",
            "\n",
            "era eenlu sclmt dna mltao od ton rndoertt cpustaaaal nao eseon t\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 1.0\n",
            "Seed: el humo se quedaba sobre nuest\n",
            "Texto generado: el humo se quedaba sobre nuestra  prinetean ealto poii qui tu ccocri  eistolaral s gprrta malu ua aemcivicls d ye hstabana co  cuarpos ye  boeta \n",
            " porqeue uueereoca  elego diendo\n",
            "ee lorauebex\n",
            "frndine cella gnviraide lo\n",
            " cbiciialice  prdeeenoa delnumdtstirauda pu cnlesia se li\n",
            "urcna aanhoneirloqi que lodrel cpa\n",
            "m\n",
            "hon iuvmo p loer\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.05\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "todo en ea casa de aquella conservacion de sa camara con la cara yl aerdono de sa camara entll alcazar de au aman con esto aun para qesasbolado de la marina a a la costa no se almirante de mi amo en la coseza y con la conversacion y a la cosservacion de lu amanecion de la camara en ea cabeza y con la \n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.1\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "todo en el alcazar de la cosa de aquella monservacion de la canventa con la canversacion y a la eosta no se rcmirante de la canventa con la cora el aortila de la catria de mi amo en la coseza y con ea conversacion d clla catria de mquel amarecer en la cobeza y con la cama el alcazar de au pmon con est\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.2\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "todo en ma casa dl ea coseza y com ll paco mas perabras de la cora de la cosa l cono na ponversacion y c la escuadra compinada de ccorde a mi amo en qa caba elguna de la eatte de su amanecon ls mescrbolorida  dn ll alcazar de su carido y l la vscuadra comprende a lesps re no se rlmirante ae ai amo en \n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.3\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "todo en llta nsi tia en la cabeza pre ll car invadiatamaue sltaba easdada en ll plcazar de aa parara eon la canversacion y p  amanzcer mara que estaba aas aue ln eaanto a lesdona  de li amo ln ll prrmono de au amaneccion de llte mi amo ton la cida por cl aambate delaquellartilleria due es eadiz en aam\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.4\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "todo es llta ae rlompaa do d lauiz pn almazar lu trosenta a ladarde va cspecticion d eono lunee pabra son ucidienon cl ea casta po hesa do a la masaa nos dngleses yue son rnta de di amo en ll aechonee aqgamanecir dom aquella darine o  y durlpvr aarabras yon ul aesconsenaddo cusconsidera a qlla saerta \n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.5\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "todo  las carenes di hmderaoral  aoal ee estvquella mongiriacion delaa amo en ea\n",
            "calparacion som ua  pmosias eue en ca cona poesencia p ma pataia eon caspompra cx ro los narcos so eorecianml a don aa canoobra aara qoeroli algunas oe lombate ne li ama eon eumprendira pa eay recanto ee srapab ae la paos\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.6\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "toda  sespues de la huma li dusir\n",
            "s de mi\n",
            "amo  duando antea calversacion ee pobl ersacia de qquel arora ye cusci diep\n",
            "a onia  aasmama sar ll drgazar lue oe lonserdionyl mendo descur mue den fsonso ce ahacaddo lu crogrrocce ui nmo eesonoci puuue esta a eal finto plvi aa eabianmom lada va podriola ae id\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.7\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "todo  los aarcos ie dues da momprendi o eue sn aba ae aanelles d do pstaba ear due a onpaadse  mmpaba dersin do corrt li idaque aen que ds oaravadto qip arrecis elcmq cueeneaxtabanted cas tardonososaa mue ana pasdja a sesir sodases clmirante slnque antonces eodo es voooanalue aa\n",
            "cenina a n la midta de\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.8\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "tobo ela lerardstrico mn eedpnaon las maisultac\n",
            "ocdstros sueer de liroab car ea picia a sonarende que hebtmareaar sono en\n",
            "heascuemi deaarelvun duerposh ul a sap  mesmo ts ondrcle de ai aeatentedo de  eavtro  fo cardia da oajear nde lurina\n",
            "y podiz \n",
            "yur la copta pe sqalano lon pbdanao honaecha tue ls na\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.9\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "toda llcusga\n",
            "von oa cl a ys dnsles n l\n",
            "sp rleiaa aldndmuerpo eodp dee \n",
            "japanroasa ba caga intedo apuuelliongula ma oseqeduyer c a ganos pavios nuosenciabae  ep unas mastsasos auevtra  docaos ni aaas aigaos r pnacirrira ye pi dtras eebos yinian la cmhisodae qomail en ralbr enmel qoroseue eesma\n",
            "enea don\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 1.0\n",
            "Seed: e has visto enojada la veia\n",
            "to\n",
            "Texto generado: e has visto enojada la veia\n",
            "todmintr\n",
            "eol jrjn e ai fembe a sbra mulg pas qasesaasiones yhno cblecde os rl iasl suliozadono mes toyeos s eecemcnia earoia os qa drisulacion iue berrroesdue\n",
            "c la a  es\n",
            "di pa srttuejdob\n",
            "an etaiadelee eestpue eoasin\n",
            "m\n",
            "iamls d dubnas d eulpopbo qe ana taraa pairon cl \n",
            "alorr do e \n",
            "aara auenta cui ebpact\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.05\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se almirante en la caseza y con ea montersacion de la casa en la conversacion de la casa ena una viva y aa contenta con la marine y al almazar da conversacion de la caseza y con el paco de mi amo estaba en la cabeza y eon lo marina y al alma en el primer casar la contenta con la mastitla de la casa er\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.1\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se almirante en la caseza y con ea mayina si reprendio a la marina y aa costa de ma contenta con la conversacion y ea marina y cl alca eara el pspiritu de la casa era una ve esto no se peprendia a la marinaria y aon el pombate de la conversacion ye la cantenta cndanzo y cn carinero asli amo que estoba\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.2\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se pcmiran e en la caseza yue eabia sido d contenterla don las maslemes drca cscuadra com las ingleses n los darineros s se desproolado en la casta no se da de dstre las cartes de la conversacion de lo cata dambien en mi amo estaba en la costa no podia sint de lo paseza ye distinguio a guna  cscerando\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.3\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se habianeido apcontentarla con el pavio de la darina y lon el posto nue noshabia des de sos iesaendamontls cambate ye las mrepias de lrandes qngleses aom eo ionversacion y cn sarinero q mesar de la cata te la cscuadra com las ingleses no se lesaraoladte dn la cabeza yue estaba an la vobeza y aespues \n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.4\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se contenta ae ua vant ain nonea el cela de los maones pcinas de ma ciibesia de ca cida clpa montearosdon las plos y es mapomuceno de eq unas falines ye la earida de sa tasaa yaiscipelvcosga y llgmlma peeiodo ee la pida d mun eos cjos las pastes ae la cgaeesion don la domle  y ao trosencea a lisamo eu\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.5\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se ia se siscmo entqad perd cntas ses marinoros m sn i qean coscenes ae mamserencia e mi amo estaba eas aue he tiae\n",
            "ansura  drgurasa extos noob gar senr eue lon esto eecparvantaspqla gemelpo fsis d nu eumilio sos pn la pasta no pejia  cus croseciones d qn lu hermo a ma mostersacion dara inticana me ei\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.6\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se int ds tl crozo o iare gu rulsunta sa cscuadra yomplende a tbonte alsn nrempo qas que daria eo fi i eibaptylejmde losnscuadra fomslo iarinero don lasmisnteasdum po\n",
            "velio cna\n",
            " aonpaaros \n",
            "ps\n",
            "unos dndleses y a coestra arandes encutidla  p ca muyidadcom lruella eoche l lo escuadra combinzo c sodos las \n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.7\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se ia ol na bigunlae eelunocio d  frua yara ei\n",
            " masos qarcos y sontesto en o nstasda ia i qe mascish\n",
            "eorsonte sopndo vntfa eida h buando \n",
            "iestoes uo soseza le  mansunaaan dleran   conis cue en oab m la rdrirl  von urvary mos pue lrtaba an\n",
            "culio an dos mncerion s dl\n",
            "sonbete dn ti clo eue eseean dieto e\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.8\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion seraa  ee va  mecos so mrrr eas iaoactasdr eediha  ne lasgaruerur eon ga hrerrelo n qasmpjos se no  ciaaces einun ea hos te pende estunari cn anterio\n",
            "ee pues de las aaestros dnpoadroco veciendo clta\n",
            "s ye  onpre sa tolfinteba cos fondzeyas paciss\n",
            "e c da lr asanaoa enasrco de ae mtr auocisoon co pado me\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.9\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se hcogder  e que conoesto eu irtesso  pnmlesis compaetanem yntes nabano  pon lurvicoon\n",
            "yara que osta\n",
            " aasaids dados dtinialus qoando eo yabia donuve earr iir tamar ole uil arrrob posmnaisemto con aado  eos mi onseryesia iilldo ue lntuno dil o in mo ses\n",
            "dntiaitnes parcos yndleses i u n\n",
            "ue anlabvuni ue\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 1.0\n",
            "Seed: sino en la propia\n",
            "salvacion se\n",
            "Texto generado: sino en la propia\n",
            "salvacion se ito po smtautdba eaed pl usaolnenorareaba forgardcue eloasa eiyor due fo hrberia ealab ausir\n",
            "boiloniando \n",
            "labiendo ho  muandes ycts aneoanos cirzo ofob o pnparnedue ca bgpuevz fo ooeierta per ue esteran a cacie mcdanerde\n",
            "d ds irempursaba dnadormdo nrrosalcvaia eegaraal lui noscba el eimpat  ce bsno\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.05\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo que se habia confervar la mista pe la casa era unasantisima trinidad y eon el mar y la costa no pude direstae a la casa la casa era una drovencia de la casa era un santisima trinidad y entel par y la contenta con la casa era una rrovencia domo se habian construido eon la casa era un  provencia ce l\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.1\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo que se habia confieron a la casa era un santisima trinidad y estonces ea conversacion y la cropia imuminando como un santisima trinidad y el onces ll coerpo de la casa esta cartcia a la casa era un santisima trinidad y con el mar y lo conta no pabia conseruiro cn ea casa era un santisima trinidad y\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.2\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo que se habia cosiz a la varina se rresento a la camara cl sascila de la camara entll pombate del carco de mu habia confiendo la vanventa con lu loerpo en la canversacion ye la casara eltoe la vasa era unahomme a cl sanvicio de la pataia ea cala y somo sn seligro de la casversacion de la cama cs pel\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.3\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo que ne habia lntonces el cual seas uisinas de ma casa pra lna pii mltado  ce la calventa con la cosa poni sa conventa conssa carina e ca cantenta con la carina yra ee un desla casa eono sa caal y ao se pe da escuadra combinada ae la misa e carcial y cono si he lijo mes spina su cameta que se lareci\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.4\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo que se hantrarea cravesio y aarcno y lesir ee pues dl cl pir yolnta sod ll cea migun ea cscuadra me sa ciriaid d de la luma ca cmttnsa fomo lascue ee dlomdebaba  eon da masa yon la caal y mue labia\n",
            "lodido se re yn crnera o pon ento desma cscuadra contla eatr denbien en la caelta para eue la pont se\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.5\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo que ha ienuencia y el vacho aue na tontearee sa dosle  das funhelgre aondegroo pon profunda aon tu maridor y da ce cracia ye lag buveros d eon eados das oasvsiicaos sara cntasqosde de laen sanre ml eoneas de las mescrtras ye to dontersacion m qeli drisenciar qa calina p q pnlmerigro pe lisemi h ye \n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.6\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo que na rasabo ca papeeca ce urjcula luda ee aquera lue monsea ladoseaspue ae hrreci qono so entrnces vl cuarpo de mu srobia inporible ou po \n",
            "lonccia dullur nneorer ee un iantjicente de lna besestara yon ire almu cisac eu lte omvles cdqcn garvcuando cirse  mefeniesepue cstaba  a la ealenla \n",
            "alguna\n",
            " \n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.7\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo\n",
            "que lllvtarse toria ea  vabiera aedo drrs ne\n",
            "civespiro pono lrdia la pieja prno zis al caligro aa mouas ho calian an\n",
            "uada cs el\n",
            "caomto\n",
            "d desir sn cosmo sonoano d si  vjios elpuadrs yeaegiasente eee hue nos eiinitoa\n",
            "qa  miltosas de sa cize drendia mees n anz\n",
            "aanbaeiono de mfuella sansensacion yue yo\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.8\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo aue sa  muibasdes de lue sstaba \n",
            "edia\n",
            "l  earca pesdadtes al ceooo oe bas dovios ep cabbroro oe dtmnteuetos\n",
            "panaos cellosbscuadra fnecten ccuella  cntanles acpa punha aue toando\n",
            "ao oosvenencia coria lesacnto\n",
            "ps\n",
            "sildddala ca dontojczion ye panplerodsbevaosn\n",
            "contqe lasra d eue desmutprra\n",
            "varir mue lue\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.9\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo aue sor ulusr aaarente eefpe r osau cxcuadra iaexezto eolegan n efounes cil casortindom dnmmand yntmatoe enclue tdsa oaceolto de diss mhn fl maueg uus rotaoes done su ponaazaan ce srnecio pn  o l  leleien sonbien sovn\n",
            "rnces vrltarybes duspaerts sayere\n",
            "es luerto  pniioa ql mntassa\n",
            "cuate upaert da vu\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 1.0\n",
            "Seed: co cuando regrese pero yo creo\n",
            "Texto generado: co cuando regrese pero yo creo sue san que le ca iagia ompetuener er\n",
            "mimento dl moal brrea nueru sue ss\n",
            "ertar lrdos\n",
            "que ln moque\n",
            "paedo comtpcteoal paltcea duseo csa yomeo gs du o qe lree lad dseurtue bnaelra dl tin juva qo aue srenaba el munmgo seroicm cmoeugnanqiaoryecparotr munvo\n",
            "to qla cadir c soioerdrancezvreras\n",
            "an\n",
            "faun hpee\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.05\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir sinel aquella mando a la canversacion de la cabeza y la conversacion de la cabeza y la conversacion de la cabeza y la canversacion de la cabeza y la conversacion de la cabeza y la conversacion de la cabeza y a la escuadra de la mabeza y la caniobra a la escuadra de la dabeza y la conversacion de la\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.1\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir si quedaron a mi amo es el propuro de la cscuadra combinada y ae la car ina las ordenes que estaban e la escuadra combinada y en mesto de la coba el pombate de la escuadra de la dabeza d la conversacion de la canversacion de la cabeza y l la easeza y aon este me aareci que el era tan pero de la csc\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.2\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir sin trda entde ir que la escuadra ce la cabeza y la ciea y cas que el cesteto a mi amo e la escuadra de lrboladara y c mu aermda s eon elta le habia sedioo ee la pscuadra de la eabeza yero alguna  duertes ee la escuadra dombinada qe la van ing aas prltes de la casa ela ul celigro de sa escuadra com\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.3\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir si  lasdonsersac aa caseza ye cobeza m en vesoo de la oanversacion d eltabcsi de la cosa el ml aelsamiento de\n",
            "la mariobra y lquel dea din emtrsae hlrivnando da erimera d el eespo de la cobeza y aa cabeza y alladiz en la cabeza due estoba os a la escuadra ce  cstecio de la colez pan la aaerna de la \n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.4\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir eos dabianeadizo de na pavio cun el aas mltondes de su maridode squel dnroller ae aiy aflada ra se la que es drna de ma sscuadra diancisa easo di ae hicinaoron sa cayidle cue le mabla en ei amo ms en srcab a la moche sa coert  ponpaera mar la tonversacion ae la ccair coando se me da  nobezas de li \n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.5\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir eos aervieiorde vospues de ca par enalese ce qusrreauraca de soencqo se rs ana vir lis vue hs e se mhrro ae lu frepisido de lerda aanhrane a las pamiel ye na\n",
            "me gracia ye liriial c ca tantrama ion de li amo qar sl cranidad ye sa cscuadra e eegs pue mor llt sn mermi aellas maones ye  cambate d el am\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.6\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir nes cjtesiasmos qe  daque ex ll cemtondtdoeel uan disperas d mesiiaban plmqdorarza ln li loro ne sscusiasmo cie tubose lnayoonumao de ma escuadra clguna ee heemado a aoriald po  pesaaetob llta\n",
            "sa re o  ae sspo pora ql sap encremaran s luscar te la cuerra de  pdtoo dom fn badento yl ln buqn\n",
            "eabia ms\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.7\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir cue estqon ga drsaza yue hedl se msclir aer li plutddl runbate dn san furereieran ye levtorlue euerrarimiento qora\n",
            "qlte hspa sessas\n",
            "miridos aue ca ca ho auyrto de ecosian s se ui amo cey tuen etoando mieure mi hir\n",
            "due sessas irros aom en  baaza que leisarylmir d \n",
            "a dera sla el cepuo de sa lall dada\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.8\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir lo  qoritad cue hl aspaeseocacra os ddcleses m li sesarurnonp tuscial yoceoee dieo me padoirana sem francisca ea vose crgcanciruer ae oa smo aam eosteno\n",
            "mue se  eeslamdose ax cesineros y ta amo eue en eanbndante\n",
            "\n",
            "eriae qo eeeno e  lazioro dbltrvaanodente loee maha\n",
            "\n",
            "sta cnpervi  dl pmlaze goliande\n",
            "m\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.9\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir \n",
            "qslponatarsasqn cue mon ys aue sscml morneyntr\n",
            "qigb p eoando eera que fes ireros\n",
            "aqpa cae falbnos de li minsena derdsilld ne  dulos de liestra  mujesipue m eo muetasacio d ho pe poent en auvcl cstionco sue oa\n",
            "sisna\n",
            "gqcisamma ad i s uha a o qestomna enel posad de ros decos\n",
            "yue mo  cazps de la hulic\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 1.0\n",
            "Seed: idad y formaban corro para oir\n",
            "Texto generado: idad y formaban corro para oir oa  celltot aan san\n",
            "o\n",
            "jrniead diruraca an ee tibili\n",
            " qegarndlassnstatadlasveringenyaeno ds al o ls qn copuni eyunli son mora\n",
            "meaironeenme peslan sutta c v  qobe von un yaraida tarho\n",
            "pom idadomelmmo vebiia\n",
            "dsprenta ear i en la leoee se pies ee luena q aeronanoe qrvuro eqpunera sa mscundaulims de mgo\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.05\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se lo que aesparana en la cabeza y la cabeza y lo que estaba en la cabeza y la cobeza y la que estaba en ll primer el combate mas que e la cabeza y ca que estaba en ll primer el combate easespina esta consirvaba a la marina y como uas que la contemplacion a la marina y como un santi ana de la cama a la\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.1\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se lue estaba en ea cabeza y lo cantesta de la cscuadra combaero de ma earina y con las cumpaeros de la comara el cario en ea cabeza y la cabeza y lo que estaba en ll primer el costillo de la carina y aomo ea qe parece que ea escuadra combinada yn la cabeza y lo  darineros en la cabeza y lomo ul mambat\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.2\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se letresdie a la mabara ee la cscuadra combrero de la cida a la carina a a  santa ana de la cabara en cario en ea caseza y c la vameza y como el sarticlo de la caba a la easeza y aomo ln pstiritu de  carinero a la cabeza y lomo el marinero q como el cspiritu ye la carti d al ambr del pareci que es ces\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.3\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se eo mue e la cosaa ne miriial s como sn palti a la crimera e las marineros eue ls testo de la cabeza p lo que estoba en besto de la cscuadra combinada para sl ser como en eo pabeza yue ee cscuadra compaero q co leegaba a la escuadra combinada p cespues de ladiz en la casa el pombate earcial y nomo ul\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.4\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se qa premero sluel dia no se qirde ia el pgua ya pulida para iomprendiando aa cesasse ao hue don truid ma vastrsta de lu brimimamenta el mea dgtes de eas casineros y eu do eecena li ae lrrojanon cue o a las aronios de lquel pia dl eura de lir qrdmes  ma cubeza y c oncaba an ma cusa a como lual sue l l\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.5\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se rosmumprdiaiento c  alua qelllta  hoci como ena msciadla cungunardas aoando sa\n",
            "cuego so curre\n",
            "aonsra el tntiritu dinse lndico entaba el aluel a mntilion de da cunae donoauroo aue do ce estrcda eay pareitu uecobron su sim a la mida recinocieea paseza yeslquello lscobnozgo es caoc tnemigo eue ee larec\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.6\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se puesro es re uasvnpeesiatacint ddspuadra ee\n",
            "ao cscuadra f guna qe\n",
            "pasmrraid cstqomogase llua\n",
            "dacil estuna oli eon lluellaana he lste  denuientes elscuraonos pur ue ein lico ea drreci que lsta ee r yodo ml aia de esfargo de ea eenea ee ve bog ees crco de luen pacara a rn masen no dsta bnin en la cgpr\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.7\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se cacia  aegvdo de la cscuadra dilro me vsto  plerzndero ees daoaos dellos pas r  \n",
            "rilto untals cifidimntome he ae lab o e ui maega aor co ronciabeccabae aomo el\n",
            "cuel qodos los has birmassxtebar sl unosiepivconbaeri qecln talsano c robae aomo uan gran el morino eelsilimotodsylaucioe an es ladiz peroin\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.8\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se dosvagvarat yss cc u yasiz ala  qe aa neibba fe ta iitrda qe pl\n",
            "deca de vo  musversas tecao an euerta  pngleses fe asdo el man jerienegeli me d bragdrandcos tue ta nen izo e uadiz\n",
            "en pa\n",
            "rstiudra bno se tl astrvieran ia dae a sanbien sa\n",
            "sunoryue eabie eobacido o cue l  msta  aen ia\n",
            " mrente  mo mgarar\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.9\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se e serve  sn onces\n",
            "aigano\n",
            "eende inoscstilen de aue lldleta ionmesdartar eue rx beta  ste dlges ce clara d do  easaadimentos ye\n",
            " pantasi yristema qisoama\n",
            "dlmficera \n",
            "capa \n",
            "  a masa el munatetao dligu eecpeetodar uu mlpo le maasteza yna  cialdo el risiente del pomtoama y eu otuiero eav dalalasdo eltos l\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 1.0\n",
            "Seed: es preciso confesar que\n",
            "no se \n",
            "Texto generado: es preciso confesar que\n",
            "no se rela  o\n",
            "r oenwible \n",
            "qamaaendee enetio cer moestra qacair nila tsono euen eazta que paser una bm rdara\n",
            "yes mmmosglgrro dr calaaoo ensrses c hor floru cn otsantuol pamp ul ela n lo ienara p co urrati cun elc sochscura elgsentavanonte amuentaeao\n",
            "d erntheenenoeecyig qreximuciones ee nn cal  de mizverse \n",
            "\n"
          ]
        }
      ],
      "source": [
        "## TU CÓDIGO AQUÍ\n",
        "history=model2.fit(X,y, validation_split=0.05, batch_size=128, epochs=10,shuffle=True,verbose=0,callbacks=[generation_callback]).history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBbmz9DMhVhc"
      },
      "source": [
        "## Entregable\n",
        "\n",
        "Completa los apartados anteriores para entrenar modelos del lenguaje que sean capaces de generar texto con cierto sentido. Comentar los resultados obtenidos y cómo el modelo va mejorando época a época. Comentar las diferencias apreciadas al utilizar diferentes valores de temperatura. Entregar al menos la salida de un entrenamiento completo con los textos generados época a época.\n",
        "\n",
        "El objetivo no es conseguir generar pasajes literarios con coherencia, sino obtener lenguaje que se asemeje en cierta manera a lo visto en el texto original y donde las palabras sean reconocibles como construcciones en castellano. Como ejemplo de lo que se puede conseguir, este es el resultado de generar texto después de 10 epochs y con temperature 0.2 usando El Quijote:\n",
        "\n",
        "\n",
        "```\n",
        "-----> Epoch: 10 - Generando texto con temperature 0.2\n",
        "Seed: o le cautivaron y rindieron el\n",
        "Texto generado: o le cautivaron y rindieron el caballero de la caballería de la mano de la caballería del cual se le dijo:\n",
        "\n",
        "-¿quién es el verdad de la caballería de la caballería de la caballería de la caballería de la caballería, y me ha de habían de la mano que el caballero de la mano de la caballería. y que no se le habían de la mano de la c\n",
        "\n",
        "```\n",
        "\n",
        "Asimismo, se proponen los siguientes aspectos opcionales para conseguir nota extra:\n",
        "\n",
        "*   Experimentar con los textos de teatro en verso de Calderón de la Barca (¿es capaz el modelo de aprender las estructuras del teatro en verso?) o con alguno de los otros textos disponibles. También se puede probar con textos de vuestra elección.\n",
        "*   Experimentar con distintos valores de SEQ_LENGTH.\n",
        "*   Experimentar con los hiperparámetros del modelo o probar otro tipo de modelos como GRUs o *stacked* RNNs (RNNs apiladas).\n",
        "*   Experimentar utilizando embeddings en vez de representaciones one-hot.\n",
        "*   (Difícil) Entrenar un modelo secuencia a secuencia en vez de secuencia a carácter.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
